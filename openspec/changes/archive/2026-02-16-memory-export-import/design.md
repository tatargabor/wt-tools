## Context

Project memory (shodh-memory) stores per-project knowledge in local RocksDB via a Rust/PyO3 library. The `wt-memory` bash CLI wraps the Python API. The GUI's Memory Browse Dialog (`gui/dialogs/memory_dialog.py`) provides search, browse, and manual note-saving. There is no way to transfer memories between machines.

Key constraint: `Memory.remember()` auto-generates UUIDs — there is no API to insert a record with a specific ID. The `metadata` dict field on each record can store arbitrary key-value pairs.

## Goals / Non-Goals

**Goals:**
- Full project export to a single JSON file
- Import with UUID-based dedup (skip duplicates, never create dupes)
- Dry-run import preview
- CLI commands (`wt-memory export`, `wt-memory import`)
- GUI buttons in Memory Browse Dialog
- Roundtrip safety: A→export→B→import→B→export→A→import produces no duplicates

**Non-Goals:**
- Selective export (by tag, type, date) — full project only for now
- Merge/conflict resolution — skip-only strategy
- Cross-project import — import always targets current project
- Streaming/incremental sync — this is manual file-based transfer
- Modifying the shodh-memory Rust library

## Decisions

### 1. Export format: Single JSON with metadata header

```json
{
  "version": 1,
  "format": "wt-memory-export",
  "project": "wt-tools",
  "exported_at": "2026-02-15T19:30:00Z",
  "count": 20,
  "records": [
    {
      "id": "abc-...",
      "content": "...",
      "experience_type": "Decision",
      "tags": ["source:user"],
      "importance": 0.5,
      "created_at": "2026-02-15T...",
      "last_accessed": "2026-02-15T...",
      "access_count": 3,
      "is_anomaly": false,
      "is_failure": false,
      "compressed": false,
      "metadata": {},
      "entities": []
    }
  ]
}
```

**Rationale**: Single JSON is simplest, human-readable, and fits the expected dataset size (tens to low hundreds of records). NDJSON would be better for streaming large datasets, but that's not our use case.

**Alternatives considered**: NDJSON (overkill), SQLite dump (not portable enough), tar of raw RocksDB (not cross-version safe).

### 2. Dedup strategy: UUID tracking via metadata.original_id

Since `remember()` generates new UUIDs, imported records get a new `id` but store the source ID in `metadata.original_id`.

On import, build a set of "known IDs" from the target project:
- All current record IDs
- All `metadata.original_id` values from previously imported records

Skip any incoming record whose `id` OR `metadata.original_id` appears in the known set.

**Decision tree**:
```
incoming.id ∈ known_ids?           → SKIP
incoming.metadata.original_id ∈ known_ids?  → SKIP
else → IMPORT (set metadata.original_id = incoming.id)
```

This handles all roundtrip scenarios without needing content hashing or timestamp comparison.

**Alternatives considered**: Content-hash dedup (fragile if content is edited), last-write-wins (data loss risk), full merge (too complex for v1).

### 3. Import preserves content and type, recalculates importance

Imported records use `remember()` with the original `content`, `experience_type`, `tags`, `entities`, and `metadata` (plus `original_id`). Fields like `importance`, `access_count`, and timestamps are regenerated by the library — this is acceptable because importance adapts to local usage patterns.

### 4. GUI placement: Top bar buttons in Memory Browse Dialog

Export and Import buttons go in the top button bar, after the existing Show All/Summary toggle. They use the `helpers.py` file picker wrappers for macOS always-on-top safety.

- Export: `get_existing_directory()` → auto-named file `<project>-memory-<date>.json`
- Import: `get_open_filename()` with JSON filter → parse, import, show result dialog

### 5. CLI output: JSON result on stdout

Both export and import output structured JSON for scriptability:
- Export: the full export JSON (or writes to `--output` file)
- Import: `{"imported": 5, "skipped": 12, "errors": 0}`
- Dry-run: `{"would_import": 5, "would_skip": 12, "dry_run": true}`

## Risks / Trade-offs

- **[Risk] Importance loss on import** → Acceptable: importance recalibrates based on local recall patterns. Original importance is in the export file if manual recovery is needed.
- **[Risk] Large export files** → Low risk: even 500 records is ~1MB JSON. Not a concern for v1.
- **[Risk] Concurrent import/export** → Mitigated: uses existing `run_with_lock` mechanism in wt-memory CLI.
- **[Risk] Forward compatibility** → `"version": 1` field allows future format changes. Import rejects unknown versions with clear error.
