#!/usr/bin/env bash
# wt-memory - CLI helper for Shodh-Memory (Python library)
# Provides per-project remember/recall operations with graceful degradation.
# If shodh-memory is not installed, all commands silently succeed (no-op).

set -euo pipefail

# Source wt-common.sh for shared functions (find_python, find_shodh_python, etc.)
_wt_memory_script="${BASH_SOURCE[0]}"
while [[ -L "$_wt_memory_script" ]]; do
    _link=$(readlink "$_wt_memory_script")
    if [[ "$_link" == /* ]]; then _wt_memory_script="$_link"; else _wt_memory_script="$(dirname "$_wt_memory_script")/$_link"; fi
done
_wt_memory_bin_dir="$(cd "$(dirname "$_wt_memory_script")" && pwd)"
# shellcheck source=wt-common.sh
source "$_wt_memory_bin_dir/wt-common.sh"

# Resolve the Python binary that has shodh-memory installed.
# Cached for the lifetime of this script invocation.
SHODH_PYTHON=""
if _sp=$(find_shodh_python 2>/dev/null); then
    SHODH_PYTHON="$_sp"
fi

# Storage root (override with env var). Per-project dirs created underneath.
SHODH_STORAGE="${SHODH_STORAGE:-${HOME}/.local/share/wt-tools/memory}"

# Global --project override (set by main before dispatch)
PROJECT=""

usage() {
    cat <<EOF
Usage: wt-memory <command> [options]

CLI helper for Shodh-Memory (local per-project cognitive memory).

Core Commands:
  health                          Check if shodh-memory is available
  remember --type TYPE [--tags t1,t2] [--metadata JSON] [--failure] [--anomaly] < content
                                  Save a memory (reads content from stdin)
  recall "query" [--limit N] [--mode MODE] [--tags t1,t2] [--tags-only] [--min-importance F]
                                  Semantic search (JSON stdout)
  recall --since ISO [--until ISO] [--limit N]
                                  Date-range recall (JSON stdout)
  proactive "context" [--limit N] Context-aware retrieval with relevance scores (JSON stdout)
  list [--type TYPE] [--limit N]  List memories (JSON stdout)
  status [--json]                 Show config, health, and memory count
  projects                        List all projects with memory counts

Forget / Cleanup:
  forget <id>                     Delete a single memory by ID
  forget --all --confirm          Delete ALL memories (requires --confirm)
  forget --older-than <days>      Delete memories older than N days
  forget --tags <t1,t2>           Delete memories matching tags
  forget --pattern <regex>        Delete memories matching regex pattern
  forget --since ISO [--until ISO] --confirm
                                  Delete memories within date range

Metrics & Reporting:
  metrics [--since Nd] [--json]   Injection quality report (default: last 7 days)
  metrics --enable                Enable metrics collection
  metrics --disable               Disable metrics collection
  dashboard [--since Nd]          Generate HTML dashboard and open in browser

Diagnostics:
  stats [--json]                  Memory quality diagnostics (types, tags, importance)
  cleanup [--threshold F] [--min-length N] [--dry-run]
                                  Remove garbage (<N chars, default 20) and low-value memories (default threshold 0.2)
  audit [--threshold F] [--json]  Duplicate detection report (default threshold 0.75)
  dedup [--threshold F] [--dry-run] [--interactive]
                                  Remove duplicate memories (keeps best per cluster)

Introspection:
  get <id>                        Get a single memory by ID (JSON stdout)
  context [topic]                 Condensed summary by category
  brain                           3-tier memory visualization
  consolidation [--since ISO] [--events]
                                  Memory consolidation report (strengthening/decay)
  graph-stats                     Knowledge graph statistics

Todo:
  todo add [--tags t1,t2] < text  Save a todo (reads from stdin)
  todo list [--json]              List open todos
  todo done <id>                  Mark todo as done (deletes it)
  todo clear --confirm            Delete all todos

Rules (deterministic operational constraints):
  rules add --topics "t1,t2" "content"  Add a rule injected when topics match prompt
  rules list                            List all rules
  rules remove <id>                     Remove a rule by id

Export / Import:
  export [--output FILE]          Export all memories to JSON (stdout or file)
  import FILE [--dry-run]         Import memories from JSON (skip duplicates)

Sync (team sharing via git):
  sync                            Push + pull in one step
  sync push                       Export and push to git remote
  sync pull [--from user/machine] Pull and import from git remote
  sync status                     Show sync state and remote sources

Migration:
  migrate                         Run pending memory migrations
  migrate --status                Show migration history

Maintenance:
  health --index                  Check index health (JSON stdout)
  verify                          Verify index integrity (find orphans)
  repair                          Repair index integrity
  flush                           Flush pending writes to disk

Recall modes (--mode):
  semantic     Meaning-based search (default)
  temporal     Time-based relevance
  hybrid       Combined semantic + temporal
  causal       Cause-effect relationships
  associative  Related concept discovery

Valid types for --type:
  Decision    A technical choice or preference
  Learning    A discovered pattern, gotcha, or lesson
  Context     Background info, events, or general notes

  Legacy aliases (mapped automatically with a warning):
    Observation → Learning
    Event       → Context

Global options:
  --project NAME                  Override project name (default: auto-detect from git root)

Environment:
  SHODH_STORAGE   Storage root (default: ~/.local/share/wt-tools/memory)

Graceful degradation:
  If shodh-memory is not installed, commands exit silently (exit 0)
  with empty JSON output.
EOF
}

# Resolve project name from git root or --project flag.
# For worktrees, resolves to the main repo name so all worktrees share memory.
resolve_project() {
    if [[ -n "$PROJECT" ]]; then
        echo "$PROJECT"
        return 0
    fi

    # Auto-detect from git root
    local toplevel
    toplevel=$(git rev-parse --show-toplevel 2>/dev/null) || true
    if [[ -z "$toplevel" ]]; then
        echo "_global"
        return 0
    fi

    # Check if this is a worktree — git-common-dir points to main repo's .git
    local common_dir
    common_dir=$(git rev-parse --git-common-dir 2>/dev/null) || true

    if [[ -n "$common_dir" && "$common_dir" != ".git" ]]; then
        # Worktree: common_dir is /path/to/main-repo/.git (absolute or relative)
        local abs_common
        abs_common=$(cd "$toplevel" && cd "$common_dir" && pwd)
        basename "$(dirname "$abs_common")"
    else
        # Main repo
        basename "$toplevel"
    fi
}

# Get current git branch name, or empty string if not in a git repo / detached HEAD.
get_current_branch() {
    git branch --show-current 2>/dev/null || true
}

# Get the per-project storage path
get_storage_path() {
    local project
    project=$(resolve_project)

    # Legacy detection: if .sst files exist directly in SHODH_STORAGE root,
    # and we'd resolve to _global, use _legacy instead
    if [[ "$project" == "_global" ]] && ls "$SHODH_STORAGE"/*.sst >/dev/null 2>&1; then
        echo "$SHODH_STORAGE/_legacy"
        return 0
    fi

    echo "$SHODH_STORAGE/$project"
}

# Get the per-project log file path for error logging
get_log_path() {
    local storage_path
    storage_path=$(get_storage_path)
    mkdir -p "$storage_path"
    echo "$storage_path/wt-memory.log"
}

# Run the resolved shodh-memory Python.
# Banner suppressed at Python level (sys._shodh_star_shown).
# Stderr goes to log file for diagnosis (not /dev/null).
run_shodh_python() {
    local log_file
    log_file=$(get_log_path)
    "$SHODH_PYTHON" "$@" 2>>"$log_file"
}

# Check if a lock directory is stale (owner dead or too old).
# Returns 0 (true) if stale, 1 (false) if active.
is_lock_stale() {
    local lock_dir="$1"
    [[ -d "$lock_dir" ]] || return 1

    # Primary check: PID file with dead owner
    local pid_file="$lock_dir/pid"
    if [[ -f "$pid_file" ]]; then
        local owner_pid
        owner_pid=$(cat "$pid_file" 2>/dev/null) || true
        if [[ -n "$owner_pid" ]] && ! kill -0 "$owner_pid" 2>/dev/null; then
            return 0  # PID is dead → stale
        fi
        # If pid file empty/unreadable or PID alive → not stale
        [[ -n "$owner_pid" ]] && return 1
        # Empty pid file — fall through to age check
    fi

    # Fallback: no pid file — check age (>60s = stale)
    local lock_age
    lock_age=$(( $(date +%s) - $(stat -c %Y "$lock_dir" 2>/dev/null || stat -f %m "$lock_dir" 2>/dev/null || echo 0) ))
    if (( lock_age > 60 )); then
        return 0  # Too old with no PID → stale
    fi

    return 1  # Recent, no PID info → assume active
}

# Remove a stale lock directory with warning.
remove_stale_lock() {
    local lock_dir="$1"
    local lock_age
    lock_age=$(( $(date +%s) - $(stat -c %Y "$lock_dir" 2>/dev/null || stat -f %m "$lock_dir" 2>/dev/null || echo 0) ))
    rm -f "$lock_dir/pid" 2>/dev/null
    rmdir "$lock_dir" 2>/dev/null
    echo "wt-memory: removed stale lock (age: ${lock_age}s)" >&2
}

# Run a command under a per-project lock to prevent concurrent RocksDB access.
# Uses mkdir-based lock (portable across Linux and macOS — no flock dependency).
# Includes stale-lock recovery: detects dead owner PIDs and aged-out locks.
# Usage: run_with_lock <command> [args...]
run_with_lock() {
    local project
    project=$(resolve_project)
    local lock_dir="/tmp/wt-memory-${project}.lock"
    local deadline=$((SECONDS + 10))

    # Pre-check: remove stale lock before even trying
    if is_lock_stale "$lock_dir"; then
        remove_stale_lock "$lock_dir"
    fi

    while ! mkdir "$lock_dir" 2>/dev/null; do
        if (( SECONDS >= deadline )); then
            # Timeout — one last staleness check before giving up
            if is_lock_stale "$lock_dir"; then
                remove_stale_lock "$lock_dir"
                # Retry once
                if mkdir "$lock_dir" 2>/dev/null; then
                    break
                fi
            fi
            echo "wt-memory: lock timeout after 10s" >&2
            return 1
        fi
        sleep 0.1
    done

    # Track lock owner for staleness detection
    echo $$ > "$lock_dir/pid" 2>/dev/null

    # Ensure lock is removed on exit (including signals)
    trap 'rm -f "$lock_dir/pid" 2>/dev/null; rmdir "$lock_dir" 2>/dev/null' EXIT

    "$@"
    local rc=$?

    rm -f "$lock_dir/pid" 2>/dev/null
    rmdir "$lock_dir" 2>/dev/null
    trap - EXIT
    return $rc
}

# Check if shodh-memory Python package is importable
# No flock needed — does not open the DB, just imports.
# Stderr suppressed — expected to fail when not installed.
cmd_health() {
    local index_check=false

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --index) index_check=true; shift ;;
            *) shift ;;
        esac
    done

    if [[ -z "$SHODH_PYTHON" ]]; then
        if [[ "$index_check" == "true" ]]; then
            echo "{}"
        fi
        return 1
    fi

    # Warn about known-broken versions
    local shodh_ver
    shodh_ver=$("$SHODH_PYTHON" -c "import sys; sys._shodh_star_shown = True; from shodh_memory import __version__; print(__version__)" 2>/dev/null) || true
    if [[ "$shodh_ver" == "0.1.80" ]]; then
        echo "Warning: shodh-memory 0.1.80 has a broken native module." >&2
        echo "Downgrade: pip install 'shodh-memory>=0.1.75,!=0.1.80'" >&2
        if [[ "$index_check" == "true" ]]; then
            echo "{}"
        fi
        return 1
    fi

    if [[ "$index_check" == "true" ]]; then
        local storage_path
        storage_path=$(get_storage_path)
        _SHODH_STORAGE="$storage_path" \
        run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
result = {}
try:
    result['index_health'] = m.index_health()
except AttributeError:
    result['index_health'] = {'error': 'index_health not available in this shodh-memory version'}
try:
    result['verify'] = m.verify_index()
except AttributeError:
    pass
print(json.dumps(result, default=str))
" || echo "{}"
    else
        echo "ok"
    fi
    return 0
}

# Save a memory: reads content from stdin
# Usage: echo "content" | wt-memory remember --type Decision --tags repo,change
cmd_remember() {
    local memory_type=""
    local tags=""
    local metadata=""
    local is_failure=false
    local is_anomaly=false

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --type) memory_type="$2"; shift 2 ;;
            --tags) tags="$2"; shift 2 ;;
            --metadata) metadata="$2"; shift 2 ;;
            --failure) is_failure=true; shift ;;
            --anomaly) is_anomaly=true; shift ;;
            *) shift ;;
        esac
    done

    if [[ -z "$memory_type" ]]; then
        echo "Error: --type is required" >&2
        return 1
    fi

    # Validate --metadata is valid JSON dict if provided
    if [[ -n "$metadata" ]]; then
        if ! echo "$metadata" | python3 -c "import sys,json; d=json.load(sys.stdin); assert isinstance(d,dict)" 2>/dev/null; then
            echo "Error: --metadata must be a valid JSON object (e.g., '{\"key\":\"value\"}')" >&2
            return 1
        fi
    fi

    # Map unsupported types to valid shodh-memory types
    case "$memory_type" in
        Observation|observation)
            echo "Note: type '$memory_type' mapped to 'Learning'" >&2
            memory_type="Learning"
            ;;
        Event|event)
            echo "Note: type '$memory_type' mapped to 'Context'" >&2
            memory_type="Context"
            ;;
    esac

    # Read content from stdin
    local content
    content=$(cat)

    if [[ -z "$content" ]]; then
        return 0
    fi

    # Health check — silent no-op if not installed
    if ! cmd_health >/dev/null 2>&1; then
        return 0
    fi

    auto_migrate

    local storage_path
    storage_path=$(get_storage_path)
    mkdir -p "$storage_path"

    # Auto-tag with current branch
    local branch
    branch=$(get_current_branch)
    if [[ -n "$branch" ]]; then
        # Only add if no branch:* tag already present
        if ! echo ",$tags," | grep -q ",branch:"; then
            if [[ -n "$tags" ]]; then
                tags="${tags},branch:${branch}"
            else
                tags="branch:${branch}"
            fi
        fi
    fi

    # Build tags as Python list
    local tags_py="[]"
    if [[ -n "$tags" ]]; then
        tags_py=$(echo "$tags" | tr ',' '\n' | jq -R . | jq -s .)
    fi

    # Pass data via env vars to avoid shell escaping issues
    local rc=0
    _SHODH_STORAGE="$storage_path" \
    _SHODH_CONTENT="$content" \
    _SHODH_TYPE="$memory_type" \
    _SHODH_TAGS="$tags_py" \
    _SHODH_METADATA="${metadata:-"{}"}" \
    _SHODH_IS_FAILURE="$is_failure" \
    _SHODH_IS_ANOMALY="$is_anomaly" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
tags = json.loads(os.environ['_SHODH_TAGS'])
metadata_raw = json.loads(os.environ.get('_SHODH_METADATA', '{}'))
metadata = {str(k): str(v) for k, v in metadata_raw.items()} if metadata_raw else None
is_failure = os.environ.get('_SHODH_IS_FAILURE', 'false') == 'true'
is_anomaly = os.environ.get('_SHODH_IS_ANOMALY', 'false') == 'true'
m.remember(os.environ['_SHODH_CONTENT'], memory_type=os.environ['_SHODH_TYPE'], tags=tags,
    metadata=metadata, is_failure=is_failure, is_anomaly=is_anomaly)
" || rc=$?

    if [[ $rc -ne 0 ]]; then
        echo "wt-memory remember: failed (exit $rc), see $(get_log_path)" >&2
    fi
    return 0
}

# Forget (delete) memories
# Usage: wt-memory forget <id>
#        wt-memory forget --all --confirm
#        wt-memory forget --older-than <days>
#        wt-memory forget --tags <t1,t2>
#        wt-memory forget --pattern <regex>
cmd_forget() {
    local memory_id=""
    local forget_all=false
    local confirm=false
    local older_than=""
    local tags=""
    local pattern=""
    local since=""
    local until=""

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --all) forget_all=true; shift ;;
            --confirm) confirm=true; shift ;;
            --older-than) older_than="$2"; shift 2 ;;
            --tags) tags="$2"; shift 2 ;;
            --pattern) pattern="$2"; shift 2 ;;
            --since) since="$2"; shift 2 ;;
            --until) until="$2"; shift 2 ;;
            -*)
                echo "Error: Unknown option '$1'" >&2
                return 1
                ;;
            *)
                if [[ -z "$memory_id" ]]; then
                    memory_id="$1"
                fi
                shift
                ;;
        esac
    done

    # Health check — silent no-op if not installed
    if ! cmd_health >/dev/null 2>&1; then
        return 0
    fi

    auto_migrate

    local storage_path
    storage_path=$(get_storage_path)

    # forget --all --confirm
    if [[ "$forget_all" == "true" ]]; then
        if [[ "$confirm" != "true" ]]; then
            echo "Error: --all requires --confirm to prevent accidental deletion" >&2
            return 1
        fi
        local rc=0
        _SHODH_STORAGE="$storage_path" \
        run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
count = m.forget_all()
print(json.dumps({'deleted_count': count}))
" || rc=$?
        return 0
    fi

    # forget --older-than <days>
    if [[ -n "$older_than" ]]; then
        _SHODH_STORAGE="$storage_path" \
        _SHODH_DAYS="$older_than" \
        run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
count = m.forget_by_age(int(os.environ['_SHODH_DAYS']))
print(json.dumps({'deleted_count': count}))
" || echo '{"deleted_count": 0}'
        return 0
    fi

    # forget --tags <t1,t2>
    if [[ -n "$tags" ]]; then
        local tags_py
        tags_py=$(echo "$tags" | tr ',' '\n' | jq -R . | jq -s .)
        _SHODH_STORAGE="$storage_path" \
        _SHODH_TAGS="$tags_py" \
        run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
tags = json.loads(os.environ['_SHODH_TAGS'])
count = m.forget_by_tags(tags)
print(json.dumps({'deleted_count': count}))
" || echo '{"deleted_count": 0}'
        return 0
    fi

    # forget --pattern <regex>
    if [[ -n "$pattern" ]]; then
        _SHODH_STORAGE="$storage_path" \
        _SHODH_PATTERN="$pattern" \
        run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
count = m.forget_by_pattern(os.environ['_SHODH_PATTERN'])
print(json.dumps({'deleted_count': count}))
" || echo '{"deleted_count": 0}'
        return 0
    fi

    # forget --since/--until (date range delete)
    if [[ -n "$since" || -n "$until" ]]; then
        if [[ "$confirm" != "true" ]]; then
            echo "Error: date-range forget requires --confirm" >&2
            return 1
        fi
        _SHODH_STORAGE="$storage_path" \
        _SHODH_SINCE="$since" \
        _SHODH_UNTIL="$until" \
        run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from datetime import datetime, timezone
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
since = os.environ.get('_SHODH_SINCE', '') or None
until = os.environ.get('_SHODH_UNTIL', '') or None

if not since:
    since = '2000-01-01T00:00:00Z'
if not until:
    until = datetime.now(timezone.utc).isoformat()

if hasattr(m, 'forget_by_date'):
    count = m.forget_by_date(since, until)
    print(json.dumps({'deleted_count': count}))
else:
    print(json.dumps({'error': 'forget_by_date not available in this shodh-memory version'}))
" || echo '{"deleted_count": 0}'
        return 0
    fi

    # forget <id> — single memory delete
    if [[ -n "$memory_id" ]]; then
        _SHODH_STORAGE="$storage_path" \
        _SHODH_ID="$memory_id" \
        run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
result = m.forget(os.environ['_SHODH_ID'])
print(json.dumps({'deleted': bool(result), 'id': os.environ['_SHODH_ID']}))
" || echo "{\"deleted\": false, \"id\": \"$memory_id\"}"
        return 0
    fi

    echo "Error: specify a memory ID, or use --all, --older-than, --tags, --pattern, or --since/--until" >&2
    return 1
}

# Semantic search
# Usage: wt-memory recall "query" --limit 5 --mode hybrid --tags t1,t2
cmd_recall() {
    local query=""
    local limit=5
    local mode=""
    local tags=""
    local tags_only=false
    local min_importance=""
    local since=""
    local until=""

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --limit) limit="$2"; shift 2 ;;
            --mode) mode="$2"; shift 2 ;;
            --tags) tags="$2"; shift 2 ;;
            --tags-only) tags_only=true; shift ;;
            --min-importance) min_importance="$2"; shift 2 ;;
            --since) since="$2"; shift 2 ;;
            --until) until="$2"; shift 2 ;;
            -*)
                shift
                ;;
            *)
                if [[ -z "$query" ]]; then
                    query="$1"
                fi
                shift
                ;;
        esac
    done

    # Date-range recall (recall_by_date)
    if [[ -n "$since" || -n "$until" ]]; then
        if ! cmd_health >/dev/null 2>&1; then
            echo "[]"
            return 0
        fi
        auto_migrate
        local storage_path
        storage_path=$(get_storage_path)

        _SHODH_STORAGE="$storage_path" \
        _SHODH_SINCE="$since" \
        _SHODH_UNTIL="$until" \
        _SHODH_LIMIT="$limit" \
        _SHODH_QUERY="$query" \
        run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from datetime import datetime, timezone
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
since = os.environ.get('_SHODH_SINCE', '') or None
until = os.environ.get('_SHODH_UNTIL', '') or None
limit = int(os.environ['_SHODH_LIMIT'])
query = os.environ.get('_SHODH_QUERY', '')

# Default open ends
if not since:
    since = '2000-01-01T00:00:00Z'
if not until:
    until = datetime.now(timezone.utc).isoformat()

if hasattr(m, 'recall_by_date'):
    results = m.recall_by_date(since, until, limit=limit)
else:
    # Fallback: use regular recall and post-filter
    results = m.recall(query or 'date range', limit=limit * 3)
    from datetime import datetime as dt
    s = dt.fromisoformat(since.replace('Z', '+00:00'))
    u = dt.fromisoformat(until.replace('Z', '+00:00'))
    filtered = []
    for r in results:
        ts = r.get('timestamp', r.get('created_at', ''))
        if ts:
            try:
                t = dt.fromisoformat(str(ts).replace('Z', '+00:00'))
                if s <= t <= u:
                    filtered.append(r)
            except (ValueError, TypeError):
                pass
    results = filtered[:limit]

print(json.dumps(results[:limit], default=str))
" || echo "[]"
        return 0
    fi

    # --tags-only requires --tags
    if [[ "$tags_only" == "true" && -z "$tags" ]]; then
        echo "Error: --tags-only requires --tags" >&2
        return 1
    fi

    if [[ -z "$query" && "$tags_only" == "false" ]]; then
        echo "[]"
        return 0
    fi

    # Health check — return empty array if not installed
    if ! cmd_health >/dev/null 2>&1; then
        echo "[]"
        return 0
    fi

    auto_migrate

    local storage_path
    storage_path=$(get_storage_path)

    # Branch-boosted recall: if no explicit --tags and on a branch,
    # issue two queries (branch-filtered + unfiltered) and merge results.
    local branch=""
    if [[ -z "$tags" ]]; then
        branch=$(get_current_branch)
    fi

    if [[ -n "$branch" ]]; then
        # Double-query: branch-filtered first, then unfiltered
        local branch_limit=$(( (limit + 1) / 2 + 1 ))
        local branch_tags_py
        branch_tags_py=$(echo "branch:$branch" | jq -R . | jq -s .)

        _SHODH_STORAGE="$storage_path" \
        _SHODH_QUERY="$query" \
        _SHODH_LIMIT="$limit" \
        _SHODH_BRANCH_LIMIT="$branch_limit" \
        _SHODH_MODE="$mode" \
        _SHODH_BRANCH_TAGS="$branch_tags_py" \
        _SHODH_MIN_IMPORTANCE="$min_importance" \
        run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
limit = int(os.environ['_SHODH_LIMIT'])
branch_limit = int(os.environ['_SHODH_BRANCH_LIMIT'])
mode = os.environ.get('_SHODH_MODE', '')
branch_tags = json.loads(os.environ['_SHODH_BRANCH_TAGS'])
min_imp = os.environ.get('_SHODH_MIN_IMPORTANCE', '')

kwargs_branch = {'limit': branch_limit, 'tags': branch_tags}
kwargs_all = {'limit': limit}
if mode:
    kwargs_branch['mode'] = mode
    kwargs_all['mode'] = mode

# Query 1: branch-specific
branch_results = m.recall(os.environ['_SHODH_QUERY'], **kwargs_branch)
# Query 2: unfiltered
all_results = m.recall(os.environ['_SHODH_QUERY'], **kwargs_all)

# Merge: branch first, then fill with unfiltered (dedup by id)
seen = set()
merged = []
for r in branch_results:
    rid = r.get('id', '')
    if rid not in seen:
        seen.add(rid)
        merged.append(r)
for r in all_results:
    rid = r.get('id', '')
    if rid not in seen:
        seen.add(rid)
        merged.append(r)

# Post-filter by min importance
if min_imp:
    threshold = float(min_imp)
    merged = [r for r in merged if float(r.get('importance', 0)) >= threshold]

print(json.dumps(merged[:limit], default=str))
" || echo "[]"
    else
        # No branch or explicit tags: single query (current behavior)
        local tags_py="[]"
        if [[ -n "$tags" ]]; then
            tags_py=$(echo "$tags" | tr ',' '\n' | jq -R . | jq -s .)
        fi

        _SHODH_STORAGE="$storage_path" \
        _SHODH_QUERY="$query" \
        _SHODH_LIMIT="$limit" \
        _SHODH_MODE="$mode" \
        _SHODH_TAGS="$tags_py" \
        _SHODH_TAGS_ONLY="$tags_only" \
        _SHODH_MIN_IMPORTANCE="$min_importance" \
        run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
tags = json.loads(os.environ.get('_SHODH_TAGS', '[]'))
tags_only = os.environ.get('_SHODH_TAGS_ONLY', 'false') == 'true'
min_imp = os.environ.get('_SHODH_MIN_IMPORTANCE', '')

if tags_only and tags:
    # Fast tag-based lookup
    if hasattr(m, 'recall_by_tags'):
        results = m.recall_by_tags(tags, limit=int(os.environ['_SHODH_LIMIT']))
    else:
        # Fallback: use recall with tags filter
        results = m.recall('', limit=int(os.environ['_SHODH_LIMIT']), tags=tags)
else:
    kwargs = {'limit': int(os.environ['_SHODH_LIMIT'])}
    mode = os.environ.get('_SHODH_MODE', '')
    if mode:
        kwargs['mode'] = mode
    if tags:
        kwargs['tags'] = tags
    results = m.recall(os.environ['_SHODH_QUERY'], **kwargs)

# Post-filter by min importance
if min_imp:
    threshold = float(min_imp)
    results = [r for r in results if float(r.get('importance', 0)) >= threshold]

print(json.dumps(results, default=str))
" || echo "[]"
    fi

    return 0
}

# Proactive context retrieval — auto-surfaces relevant memories with relevance scores
# Usage: wt-memory proactive "conversation context" [--limit N]
cmd_proactive() {
    local context=""
    local limit=5

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --limit) limit="$2"; shift 2 ;;
            -*)
                shift
                ;;
            *)
                if [[ -z "$context" ]]; then
                    context="$1"
                fi
                shift
                ;;
        esac
    done

    if [[ -z "$context" ]]; then
        echo "[]"
        return 0
    fi

    # Health check — return empty array if not installed
    if ! cmd_health >/dev/null 2>&1; then
        echo "[]"
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)

    _SHODH_STORAGE="$storage_path" \
    _SHODH_CONTEXT="$context" \
    _SHODH_LIMIT="$limit" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
limit = int(os.environ['_SHODH_LIMIT'])
context = os.environ['_SHODH_CONTEXT']

if hasattr(m, 'proactive_context'):
    raw = m.proactive_context(context, max_results=limit, auto_ingest=False, semantic_threshold=0.3)
    results = raw.get('memories', []) if isinstance(raw, dict) else raw

    # Always augment with hybrid recall to catch keyword matches that semantic misses
    hybrid = m.recall(context, limit=limit, mode='hybrid')
    seen = {r.get('content', '')[:50] for r in results}
    hybrid_new = []
    for h in hybrid:
        key = h.get('content', '')[:50]
        if key not in seen:
            seen.add(key)
            h['relevance_score'] = 0.35
            hybrid_new.append(h)

    # Reserve slots for hybrid-only results (up to 2), trim proactive if needed
    if hybrid_new:
        reserve = min(len(hybrid_new), 2)
        results = results[:max(limit - reserve, 0)] + hybrid_new[:reserve]
    results = results[:limit]
    print(json.dumps(results, default=str))
else:
    # Fallback: use recall with hybrid mode (old shodh-memory without proactive_context)
    results = m.recall(context, limit=limit, mode='hybrid')
    for r in results:
        r['relevance_score'] = 'N/A'
    print(json.dumps(results, default=str))
" || echo "[]"

    return 0
}

# List all memories for current project
# Usage: wt-memory list [--type Decision] [--limit 20]
cmd_list() {
    local memory_type=""
    local limit=""

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --type) memory_type="$2"; shift 2 ;;
            --limit) limit="$2"; shift 2 ;;
            *) shift ;;
        esac
    done

    # Health check — return empty array if not installed
    if ! cmd_health >/dev/null 2>&1; then
        echo "[]"
        return 0
    fi

    auto_migrate

    local storage_path
    storage_path=$(get_storage_path)

    # If storage dir doesn't exist yet, no memories
    if [[ ! -d "$storage_path" ]]; then
        echo "[]"
        return 0
    fi

    _SHODH_STORAGE="$storage_path" \
    _SHODH_TYPE="$memory_type" \
    _SHODH_LIMIT="$limit" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
kwargs = {}
mt = os.environ.get('_SHODH_TYPE', '')
if mt:
    kwargs['memory_type'] = mt
lim = os.environ.get('_SHODH_LIMIT', '')
if lim:
    kwargs['limit'] = int(lim)
memories = m.list_memories(**kwargs)
print(json.dumps(memories, default=str))
" || echo "[]"

    return 0
}

# Context summary by category
# Usage: wt-memory context [topic]
cmd_context() {
    local topic=""
    while [[ $# -gt 0 ]]; do
        case "$1" in
            -*) shift ;;
            *)
                if [[ -z "$topic" ]]; then
                    topic="$1"
                fi
                shift
                ;;
        esac
    done

    if ! cmd_health >/dev/null 2>&1; then
        echo "{}"
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)

    _SHODH_STORAGE="$storage_path" \
    _SHODH_TOPIC="$topic" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
topic = os.environ.get('_SHODH_TOPIC', '')
try:
    if topic:
        result = m.context_summary(max_items=5, include_decisions=True, include_learnings=True, include_context=True)
    else:
        result = m.context_summary(max_items=5, include_decisions=True, include_learnings=True, include_context=True)
    print(json.dumps(result, default=str))
except AttributeError:
    print(json.dumps({'error': 'context_summary not available in this shodh-memory version'}))
" || echo "{}"

    return 0
}

# 3-tier memory visualization
# Usage: wt-memory brain
cmd_brain() {
    if ! cmd_health >/dev/null 2>&1; then
        echo "{}"
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)

    _SHODH_STORAGE="$storage_path" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
try:
    result = m.brain_state(longterm_limit=100)
    print(json.dumps(result, default=str))
except AttributeError:
    print(json.dumps({'error': 'brain_state not available in this shodh-memory version'}))
" || echo "{}"

    return 0
}

# Memory quality diagnostics
# Usage: wt-memory stats [--json]
cmd_stats() {
    local json_mode=false

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --json) json_mode=true; shift ;;
            *) shift ;;
        esac
    done

    if ! cmd_health >/dev/null 2>&1; then
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)

    _SHODH_STORAGE="$storage_path" \
    _SHODH_JSON="$json_mode" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from collections import Counter
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
json_mode = os.environ.get('_SHODH_JSON', 'false') == 'true'

# Get all memories
memories = m.list_memories()
total = len(memories)

# Type distribution
type_dist = Counter(r.get('experience_type', 'unknown') for r in memories)

# Tag frequency (top 10)
tag_counter = Counter()
for r in memories:
    for t in r.get('tags', []):
        tag_counter[t] += 1
tag_dist = dict(tag_counter.most_common(10))

# Importance histogram (5 buckets)
buckets = {'0.0-0.2': 0, '0.2-0.4': 0, '0.4-0.6': 0, '0.6-0.8': 0, '0.8-1.0': 0}
noise_count = 0
for r in memories:
    imp = float(r.get('importance', 0.5))
    if imp < 0.2: buckets['0.0-0.2'] += 1
    elif imp < 0.4: buckets['0.2-0.4'] += 1
    elif imp < 0.6: buckets['0.4-0.6'] += 1
    elif imp < 0.8: buckets['0.6-0.8'] += 1
    else: buckets['0.8-1.0'] += 1
    if imp < 0.3: noise_count += 1

noise_ratio = round(noise_count / total, 2) if total > 0 else 0

if json_mode:
    print(json.dumps({
        'total': total,
        'type_distribution': dict(type_dist),
        'tag_distribution': tag_dist,
        'importance_histogram': buckets,
        'noise_ratio': noise_ratio
    }, default=str))
else:
    print(f'Total memories: {total}')
    print(f'Noise ratio: {noise_ratio:.0%} (importance < 0.3)')
    print()
    print('Type distribution:')
    for t, c in sorted(type_dist.items(), key=lambda x: -x[1]):
        print(f'  {t}: {c}')
    print()
    print('Top tags:')
    for t, c in tag_counter.most_common(10):
        print(f'  {t}: {c}')
    print()
    print('Importance histogram:')
    for bucket, count in buckets.items():
        bar = '#' * count
        print(f'  {bucket}: {count:3d} {bar}')
" || { [[ "$json_mode" == "true" ]] && echo "{}"; }

    return 0
}

# Memory cleanup — remove low-value and garbage memories
# Usage: wt-memory cleanup [--threshold F] [--min-length N] [--dry-run]
cmd_cleanup() {
    local threshold=0.2
    local min_length=20
    local dry_run=false

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --threshold) threshold="$2"; shift 2 ;;
            --min-length) min_length="$2"; shift 2 ;;
            --dry-run) dry_run=true; shift ;;
            *) shift ;;
        esac
    done

    if ! cmd_health >/dev/null 2>&1; then
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)

    _SHODH_STORAGE="$storage_path" \
    _SHODH_THRESHOLD="$threshold" \
    _SHODH_MIN_LENGTH="$min_length" \
    _SHODH_DRY_RUN="$dry_run" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
threshold = float(os.environ['_SHODH_THRESHOLD'])
min_length = int(os.environ.get('_SHODH_MIN_LENGTH', '20'))
dry_run = os.environ.get('_SHODH_DRY_RUN', 'false') == 'true'

memories = m.list_memories()

# Phase 1: remove garbage (content too short to be useful)
garbage = [r for r in memories if len(r.get('content', '')) < min_length]

# Phase 2: remove low-importance memories
low_value = [r for r in memories if r not in garbage and float(r.get('importance', 0.5)) < threshold]

if dry_run:
    print(json.dumps({'garbage': len(garbage), 'low_importance': len(low_value),
                       'would_delete': len(garbage) + len(low_value), 'dry_run': True}))
else:
    deleted = 0
    for r in garbage + low_value:
        try:
            m.forget(r['id'])
            deleted += 1
        except Exception:
            pass
    print(json.dumps({'deleted_count': deleted, 'garbage': len(garbage), 'low_importance': len(low_value)}))
" || echo '{"deleted_count": 0}'

    return 0
}

# Shared Python dedup engine — used by both audit and dedup commands.
# Outputs JSON with cluster analysis to stdout.
# Env vars: _SHODH_STORAGE, _SHODH_THRESHOLD, _SHODH_MODE (audit|dedup|interactive),
#           _SHODH_DRY_RUN
_DEDUP_PYTHON='
import sys; sys._shodh_star_shown = True
import json, os
from difflib import SequenceMatcher
from collections import defaultdict
from shodh_memory import Memory

storage = os.environ["_SHODH_STORAGE"]
threshold = float(os.environ.get("_SHODH_THRESHOLD", "0.75"))
mode = os.environ.get("_SHODH_MODE", "audit")
dry_run = os.environ.get("_SHODH_DRY_RUN", "false") == "true"

m = Memory(storage_path=storage)
memories = m.list_memories()

if not memories:
    if mode == "audit":
        print(json.dumps({"total": 0, "clusters": 0, "redundant": 0, "unique": 0, "dedup_ratio": 0.0, "top_clusters": []}))
    else:
        print(json.dumps({"deleted_count": 0, "merged_count": 0}))
    sys.exit(0)

# Union-find
parents = list(range(len(memories)))
def find(x):
    while parents[x] != x:
        parents[x] = parents[parents[x]]
        x = parents[x]
    return x
def union(a, b):
    pa, pb = find(a), find(b)
    if pa != pb:
        parents[pa] = pb

# Pairwise similarity
for i in range(len(memories)):
    for j in range(i + 1, len(memories)):
        ratio = SequenceMatcher(None, memories[i]["content"], memories[j]["content"]).ratio()
        if ratio > threshold:
            union(i, j)

# Build clusters
clusters = defaultdict(list)
for i in range(len(memories)):
    clusters[find(i)].append(i)
dup_clusters = {k: v for k, v in clusters.items() if len(v) > 1}

total = len(memories)
num_clusters = len(dup_clusters)
redundant = sum(len(v) - 1 for v in dup_clusters.values())
unique = total - redundant
dedup_ratio = (redundant / total * 100) if total > 0 else 0.0

# Survivor selection: composite score
def score(mem):
    return (
        mem.get("access_count", 0) * 10
        + float(mem.get("importance", 0.5)) * 5
        + len(mem.get("content", "")) / 100
    )

# Build cluster info sorted by size
sorted_clusters = sorted(dup_clusters.values(), key=lambda v: -len(v))
top_clusters = []
for indices in sorted_clusters[:10]:
    cluster_mems = [memories[i] for i in indices]
    scored = sorted(cluster_mems, key=lambda mem: (-score(mem), mem.get("created_at", "")))
    survivor = scored[0]
    victims = scored[1:]
    top_clusters.append({
        "count": len(cluster_mems),
        "preview": survivor["content"][:120],
        "survivor_id": survivor["id"],
        "ids": [mem["id"] for mem in cluster_mems],
        "victim_ids": [mem["id"] for mem in victims],
        "all_tags": list(set(t for mem in cluster_mems for t in mem.get("tags", []))),
        "survivor_tags": survivor.get("tags", []),
    })

result = {
    "total": total,
    "clusters": num_clusters,
    "redundant": redundant,
    "unique": unique,
    "dedup_ratio": round(dedup_ratio, 1),
    "top_clusters": top_clusters,
}

if mode == "audit":
    print(json.dumps(result, default=str))
    sys.exit(0)

# --- Dedup / Interactive mode ---
if dry_run:
    print(json.dumps({"dry_run": True, "clusters": num_clusters, "would_delete": redundant, "top_clusters": top_clusters}, default=str))
    sys.exit(0)

if mode == "interactive":
    # Interactive: print clusters as numbered items to stderr, read decisions from stdin
    import select
    if not sys.stdin.isatty():
        sys.stderr.write("Warning: stdin is not a TTY, falling back to dry-run\n")
        print(json.dumps({"dry_run": True, "clusters": num_clusters, "would_delete": redundant, "top_clusters": top_clusters}, default=str))
        sys.exit(0)

    deleted_count = 0
    merged_count = 0
    for ci, cluster_info in enumerate(top_clusters):
        cluster_mems = [mem for mem in memories if mem["id"] in cluster_info["ids"]]
        scored = sorted(cluster_mems, key=lambda mem: (-score(mem), mem.get("created_at", "")))
        survivor = scored[0]
        victims = scored[1:]

        sys.stderr.write(f"\n--- Cluster {ci+1}/{len(top_clusters)} ({len(cluster_mems)} entries) ---\n")
        for mi, mem in enumerate(scored):
            marker = " *BEST*" if mem["id"] == survivor["id"] else ""
            mid = mem["id"][:8]
            acc = mem.get("access_count", 0)
            imp = float(mem.get("importance", 0.5))
            preview = mem["content"][:80]
            sys.stderr.write(f"  [{mi+1}] {mid}.. | acc={acc} imp={imp:.2f} | {preview}{marker}\n")
        sys.stderr.write("  [k]eep best / [s]kip / [q]uit: ")
        sys.stderr.flush()

        choice = input().strip().lower()
        if choice == "q":
            break
        elif choice == "s":
            continue
        else:
            # Keep best (default)
            merged_tags = list(set(t for mem in cluster_mems for t in mem.get("tags", [])))
            needs_merge = set(merged_tags) != set(survivor.get("tags", []))

            for v in victims:
                try:
                    m.forget(v["id"])
                    deleted_count += 1
                except Exception:
                    pass

            if needs_merge:
                try:
                    m.forget(survivor["id"])
                    m.remember(
                        content=survivor["content"],
                        memory_type=survivor.get("experience_type", "Learning"),
                        tags=merged_tags,
                        importance=float(survivor.get("importance", 0.5)),
                        metadata=survivor.get("metadata", {}),
                        is_failure=survivor.get("is_failure", False),
                        is_anomaly=survivor.get("is_anomaly", False),
                    )
                    merged_count += 1
                except Exception as e:
                    sys.stderr.write(f"  Warning: tag merge failed: {e}\n")

    print(json.dumps({"deleted_count": deleted_count, "merged_count": merged_count}))
    sys.exit(0)

# --- Execute mode (non-interactive) ---
deleted_count = 0
merged_count = 0
for cluster_info in top_clusters:
    cluster_mems = [mem for mem in memories if mem["id"] in cluster_info["ids"]]
    scored = sorted(cluster_mems, key=lambda mem: (-score(mem), mem.get("created_at", "")))
    survivor = scored[0]
    victims = scored[1:]

    merged_tags = list(set(t for mem in cluster_mems for t in mem.get("tags", [])))
    needs_merge = set(merged_tags) != set(survivor.get("tags", []))

    for v in victims:
        try:
            m.forget(v["id"])
            deleted_count += 1
        except Exception:
            pass

    if needs_merge:
        try:
            m.forget(survivor["id"])
            m.remember(
                content=survivor["content"],
                memory_type=survivor.get("experience_type", "Learning"),
                tags=merged_tags,
                importance=float(survivor.get("importance", 0.5)),
                metadata=survivor.get("metadata", {}),
                is_failure=survivor.get("is_failure", False),
                is_anomaly=survivor.get("is_anomaly", False),
            )
            merged_count += 1
        except Exception:
            pass

print(json.dumps({"deleted_count": deleted_count, "merged_count": merged_count}))
'

# Memory audit — report duplicate clusters and memory health
# Usage: wt-memory audit [--threshold F] [--json]
cmd_audit() {
    local threshold=0.75
    local json_mode=false

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --threshold) threshold="$2"; shift 2 ;;
            --json) json_mode=true; shift ;;
            *) shift ;;
        esac
    done

    if ! cmd_health >/dev/null 2>&1; then
        if [[ "$json_mode" == "true" ]]; then
            echo '{"total": 0, "clusters": 0, "redundant": 0, "unique": 0, "dedup_ratio": 0.0, "top_clusters": []}'
        else
            echo "No memories to audit (shodh-memory not available)."
        fi
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)

    if [[ ! -d "$storage_path" ]]; then
        if [[ "$json_mode" == "true" ]]; then
            echo '{"total": 0, "clusters": 0, "redundant": 0, "unique": 0, "dedup_ratio": 0.0, "top_clusters": []}'
        else
            echo "No memories to audit (empty store)."
        fi
        return 0
    fi

    local raw_json
    raw_json=$(_SHODH_STORAGE="$storage_path" \
        _SHODH_THRESHOLD="$threshold" \
        _SHODH_MODE="audit" \
        run_with_lock run_shodh_python -c "$_DEDUP_PYTHON") || {
        [[ "$json_mode" == "true" ]] && echo '{"total": 0, "clusters": 0, "redundant": 0, "unique": 0, "dedup_ratio": 0.0, "top_clusters": []}'
        return 0
    }

    if [[ "$json_mode" == "true" ]]; then
        echo "$raw_json"
    else
        # Human-readable report
        echo "$raw_json" | python3 -c "
import json, sys
d = json.load(sys.stdin)
print('Memory Audit Report')
print('=' * 40)
print(f'Total memories:    {d[\"total\"]}')
print(f'Duplicate clusters: {d[\"clusters\"]}')
print(f'Redundant entries:  {d[\"redundant\"]}')
print(f'Unique (estimated): {d[\"unique\"]}')
print(f'Dedup ratio:        {d[\"dedup_ratio\"]}%')
if d['top_clusters']:
    print()
    print('Top duplicate clusters:')
    for c in d['top_clusters']:
        print(f'  [{c[\"count\"]}x] {c[\"preview\"]}')
else:
    print()
    print('No duplicates found.')
if d['redundant'] > 0:
    print()
    print('Run \`wt-memory dedup --dry-run\` to preview cleanup.')
"
    fi

    return 0
}

# Memory dedup — remove duplicate memories
# Usage: wt-memory dedup [--threshold F] [--dry-run] [--interactive]
cmd_dedup() {
    local threshold=0.75
    local dry_run=false
    local interactive=false

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --threshold) threshold="$2"; shift 2 ;;
            --dry-run) dry_run=true; shift ;;
            --interactive|-i) interactive=true; shift ;;
            *) shift ;;
        esac
    done

    if ! cmd_health >/dev/null 2>&1; then
        echo '{"deleted_count": 0, "merged_count": 0}'
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)

    if [[ ! -d "$storage_path" ]]; then
        echo '{"deleted_count": 0, "merged_count": 0}'
        return 0
    fi

    local mode="dedup"
    if [[ "$interactive" == "true" ]]; then
        mode="interactive"
    fi

    local shodh_dry_run="false"
    if [[ "$dry_run" == "true" ]]; then
        shodh_dry_run="true"
    fi

    _SHODH_STORAGE="$storage_path" \
    _SHODH_THRESHOLD="$threshold" \
    _SHODH_MODE="$mode" \
    _SHODH_DRY_RUN="$shodh_dry_run" \
    run_with_lock run_shodh_python -c "$_DEDUP_PYTHON" || echo '{"deleted_count": 0, "merged_count": 0}'

    return 0
}

# Export all memories to JSON
# Usage: wt-memory export [--output FILE]
cmd_export() {
    local output_file=""

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --output) output_file="$2"; shift 2 ;;
            *) shift ;;
        esac
    done

    # Health check — silent no-op if not installed
    if ! cmd_health >/dev/null 2>&1; then
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)
    local project
    project=$(resolve_project)

    local json_output
    json_output=$(_SHODH_STORAGE="$storage_path" \
    _SHODH_PROJECT="$project" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from datetime import datetime, timezone
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
records = m.list_memories()
export_data = {
    'version': 1,
    'format': 'wt-memory-export',
    'project': os.environ['_SHODH_PROJECT'],
    'exported_at': datetime.now(timezone.utc).isoformat(),
    'count': len(records),
    'records': records
}
print(json.dumps(export_data, indent=2, default=str))
") || { echo '{"version":1,"format":"wt-memory-export","count":0,"records":[]}'; return 0; }

    if [[ -n "$output_file" ]]; then
        echo "$json_output" > "$output_file"
    else
        echo "$json_output"
    fi
    return 0
}

# Import memories from JSON export file
# Usage: wt-memory import FILE [--dry-run]
cmd_import() {
    local import_file=""
    local dry_run=false

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --dry-run) dry_run=true; shift ;;
            -*)
                echo "Error: Unknown option '$1'" >&2
                return 1
                ;;
            *)
                if [[ -z "$import_file" ]]; then
                    import_file="$1"
                fi
                shift
                ;;
        esac
    done

    if [[ -z "$import_file" ]]; then
        echo "Error: import file path required" >&2
        return 1
    fi

    if [[ ! -f "$import_file" ]]; then
        echo "Error: file not found: $import_file" >&2
        return 1
    fi

    # Health check — silent no-op if not installed
    if ! cmd_health >/dev/null 2>&1; then
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)
    mkdir -p "$storage_path"

    _SHODH_STORAGE="$storage_path" \
    _SHODH_FILE="$import_file" \
    _SHODH_DRY_RUN="$dry_run" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory

# Read and validate import file
file_path = os.environ['_SHODH_FILE']
dry_run = os.environ.get('_SHODH_DRY_RUN', 'false') == 'true'

try:
    with open(file_path, 'r') as f:
        data = json.load(f)
except json.JSONDecodeError as e:
    print(json.dumps({'error': f'Invalid JSON: {e}'}))
    sys.exit(1)

# Validate format
if data.get('format') != 'wt-memory-export':
    print(json.dumps({'error': 'Invalid file: missing or wrong format field (expected wt-memory-export)'}))
    sys.exit(1)

if data.get('version', 0) != 1:
    print(json.dumps({'error': f\"Unsupported version: {data.get('version')} (only version 1 supported)\"}))
    sys.exit(1)

records = data.get('records', [])

# Open memory store and build known-ID set for dedup
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
existing = m.list_memories()

known_ids = set()
for rec in existing:
    known_ids.add(rec['id'])
    orig = rec.get('metadata', {}).get('original_id')
    if orig:
        known_ids.add(orig)

imported = 0
skipped = 0
errors = 0

for rec in records:
    # Check all dedup conditions
    rec_id = rec.get('id', '')
    rec_original_id = rec.get('metadata', {}).get('original_id', '')

    if rec_id in known_ids:
        skipped += 1
        continue
    if rec_original_id and rec_original_id in known_ids:
        skipped += 1
        continue

    if dry_run:
        imported += 1
        continue

    try:
        # Build metadata with original_id tracking
        metadata = dict(rec.get('metadata', {}) or {})
        metadata['original_id'] = rec_id

        m.remember(
            rec.get('content', ''),
            memory_type=rec.get('experience_type', 'Context'),
            tags=rec.get('tags', []),
            entities=rec.get('entities', []),
            metadata=metadata,
            is_failure=rec.get('is_failure', False),
            is_anomaly=rec.get('is_anomaly', False),
        )
        imported += 1
        # Add to known set so subsequent records in same file are deduped
        known_ids.add(rec_id)
    except Exception as e:
        errors += 1

if dry_run:
    print(json.dumps({'would_import': imported, 'would_skip': skipped, 'dry_run': True}))
else:
    print(json.dumps({'imported': imported, 'skipped': skipped, 'errors': errors}))
"
    return $?
}

# Get single memory by ID
# Usage: wt-memory get <memory_id>
cmd_get() {
    local memory_id=""
    while [[ $# -gt 0 ]]; do
        case "$1" in
            -*) shift ;;
            *)
                if [[ -z "$memory_id" ]]; then
                    memory_id="$1"
                fi
                shift
                ;;
        esac
    done

    if [[ -z "$memory_id" ]]; then
        echo "Error: memory ID required" >&2
        return 1
    fi

    if ! cmd_health >/dev/null 2>&1; then
        echo "{}"
        return 0
    fi

    auto_migrate

    local storage_path
    storage_path=$(get_storage_path)

    _SHODH_STORAGE="$storage_path" \
    _SHODH_ID="$memory_id" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
try:
    result = m.get_memory(os.environ['_SHODH_ID'])
    print(json.dumps(result, default=str))
except Exception:
    print('{}')
" || echo "{}"

    return 0
}

# Repair index integrity
# Usage: wt-memory repair
cmd_repair() {
    if ! cmd_health >/dev/null 2>&1; then
        echo "{}"
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)
    mkdir -p "$storage_path"

    _SHODH_STORAGE="$storage_path" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
try:
    result = m.repair_index()
    print(json.dumps(result, default=str))
except AttributeError:
    print(json.dumps({'error': 'repair_index not available in this shodh-memory version'}))
" || echo "{}"

    return 0
}

# ============================================================
# Rules management — deterministic per-project operational rules
# Stored in .claude/rules.yaml at the project root.
# ============================================================

# Resolve path to .claude/rules.yaml for this project.
# Uses git toplevel; falls back to $CLAUDE_PROJECT_DIR if not in a git repo.
get_rules_file() {
    local toplevel
    toplevel=$(git rev-parse --show-toplevel 2>/dev/null) || true
    if [[ -z "$toplevel" ]]; then
        if [[ -n "${CLAUDE_PROJECT_DIR:-}" ]]; then
            toplevel="$CLAUDE_PROJECT_DIR"
        else
            toplevel="$(pwd)"
        fi
    fi
    echo "${toplevel}/.claude/rules.yaml"
}

# Generate a kebab-case id from the first 4 words of a string.
_rules_make_id() {
    local content="$1"
    echo "$content" | tr '[:upper:]' '[:lower:]' | \
        tr -cs 'a-z0-9' '-' | \
        sed 's/^-*//;s/-*$//' | \
        cut -c1-40 | \
        sed 's/-\{2,\}/-/g'
}

# Read .claude/rules.yaml and print rules matching at least one topic in $1 (space-sep prompt text).
# Output format: one line per rule — "id\tcontent"
# Returns 0 even if file is absent or malformed.
_rules_match() {
    local prompt_lower="$1"
    local rules_file
    rules_file=$(get_rules_file)
    [[ -f "$rules_file" ]] || return 0

    python3 - "$rules_file" "$prompt_lower" <<'PYEOF' 2>/dev/null || true
import sys, yaml, re

rules_file = sys.argv[1]
prompt_text = sys.argv[2].lower()

try:
    with open(rules_file) as f:
        data = yaml.safe_load(f)
except Exception:
    sys.exit(0)

if not isinstance(data, dict) or not isinstance(data.get("rules"), list):
    sys.exit(0)

for rule in data["rules"]:
    if not isinstance(rule, dict):
        continue
    topics = rule.get("topics", [])
    content = rule.get("content", "").strip()
    rule_id = rule.get("id", "")
    if not topics or not content:
        continue
    for topic in topics:
        if str(topic).lower() in prompt_text:
            print(f"{rule_id}\t{content}")
            break
PYEOF
}

# cmd_rules dispatcher
cmd_rules() {
    local subcmd="${1:-}"
    shift 2>/dev/null || true

    case "$subcmd" in
        add)    cmd_rules_add "$@" ;;
        list)   cmd_rules_list "$@" ;;
        remove) cmd_rules_remove "$@" ;;
        "")
            echo "Usage: wt-memory rules <add|list|remove>" >&2
            echo "  add --topics \"t1,t2\" \"content\"   Add a rule (injected when topics match prompt)" >&2
            echo "  list                              List all rules" >&2
            echo "  remove <id>                       Remove a rule by id" >&2
            return 1
            ;;
        *)
            echo "Error: Unknown rules subcommand '$subcmd'" >&2
            return 1
            ;;
    esac
}

cmd_rules_add() {
    local topics=""
    local content=""

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --topics) topics="$2"; shift 2 ;;
            *) content="$1"; shift ;;
        esac
    done

    if [[ -z "$content" ]]; then
        echo "Error: content required (e.g., wt-memory rules add --topics \"sql,customer\" \"Use customer_ro / XYZ123\")" >&2
        return 1
    fi
    if [[ -z "$topics" ]]; then
        echo "Error: --topics required (comma-separated keywords, e.g., --topics \"sql,customer\")" >&2
        return 1
    fi

    local rules_file
    rules_file=$(get_rules_file)
    mkdir -p "$(dirname "$rules_file")"

    # Generate id from content
    local id
    id=$(_rules_make_id "$content")
    # Truncate to first 4 "words" worth (max 40 chars already handled)
    id=$(echo "$id" | cut -c1-40)

    python3 - "$rules_file" "$id" "$topics" "$content" <<'PYEOF'
import sys, yaml, os

rules_file, rule_id, topics_str, content = sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4]

topics = [t.strip() for t in topics_str.split(",") if t.strip()]

data = {"rules": []}
if os.path.exists(rules_file):
    try:
        with open(rules_file) as f:
            loaded = yaml.safe_load(f)
        if isinstance(loaded, dict) and isinstance(loaded.get("rules"), list):
            data = loaded
    except Exception:
        pass

# Ensure unique id
existing_ids = {r.get("id") for r in data["rules"] if isinstance(r, dict)}
base_id = rule_id
counter = 2
while rule_id in existing_ids:
    rule_id = f"{base_id}-{counter}"
    counter += 1

data["rules"].append({"id": rule_id, "topics": topics, "content": content})

with open(rules_file, "w") as f:
    yaml.dump(data, f, default_flow_style=False, allow_unicode=True, sort_keys=False)

print(f"Rule added: {rule_id}")
print(f"  topics: {', '.join(topics)}")
print(f"  file:   {rules_file}")
PYEOF
}

cmd_rules_list() {
    local rules_file
    rules_file=$(get_rules_file)

    if [[ ! -f "$rules_file" ]]; then
        echo "No rules file found at: $rules_file"
        echo "Use 'wt-memory rules add --topics \"t1,t2\" \"content\"' to create one."
        return 0
    fi

    python3 - "$rules_file" <<'PYEOF'
import sys, yaml

rules_file = sys.argv[1]
try:
    with open(rules_file) as f:
        data = yaml.safe_load(f)
except Exception as e:
    print(f"Error reading rules file: {e}", file=sys.stderr)
    sys.exit(1)

rules = data.get("rules", []) if isinstance(data, dict) else []
if not rules:
    print("No rules defined.")
    sys.exit(0)

print(f"Rules ({len(rules)}):")
print()
for rule in rules:
    if not isinstance(rule, dict):
        continue
    rid = rule.get("id", "(no id)")
    topics = ", ".join(rule.get("topics", []))
    content = rule.get("content", "").strip()
    preview = content[:120] + ("..." if len(content) > 120 else "")
    print(f"  [{rid}]")
    print(f"    topics:  {topics}")
    print(f"    content: {preview}")
    print()
PYEOF
}

cmd_rules_remove() {
    local rule_id="${1:-}"
    if [[ -z "$rule_id" ]]; then
        echo "Error: rule id required (use 'wt-memory rules list' to see ids)" >&2
        return 1
    fi

    local rules_file
    rules_file=$(get_rules_file)

    if [[ ! -f "$rules_file" ]]; then
        echo "Error: no rules file found at: $rules_file" >&2
        return 1
    fi

    python3 - "$rules_file" "$rule_id" <<'PYEOF'
import sys, yaml

rules_file, rule_id = sys.argv[1], sys.argv[2]

try:
    with open(rules_file) as f:
        data = yaml.safe_load(f)
except Exception as e:
    print(f"Error reading rules file: {e}", file=sys.stderr)
    sys.exit(1)

rules = data.get("rules", []) if isinstance(data, dict) else []
new_rules = [r for r in rules if isinstance(r, dict) and r.get("id") != rule_id]

if len(new_rules) == len(rules):
    print(f"Error: rule '{rule_id}' not found", file=sys.stderr)
    sys.exit(1)

data["rules"] = new_rules
with open(rules_file, "w") as f:
    yaml.dump(data, f, default_flow_style=False, allow_unicode=True, sort_keys=False)

print(f"Rule removed: {rule_id}")
PYEOF
}

# Todo management — lightweight task capture built on memory API
# Usage: wt-memory todo add|list|done|clear [options]
cmd_todo() {
    local subcmd="${1:-}"
    shift 2>/dev/null || true

    case "$subcmd" in
        add)  cmd_todo_add "$@" ;;
        list) cmd_todo_list "$@" ;;
        done) cmd_todo_done "$@" ;;
        clear) cmd_todo_clear "$@" ;;
        "")
            echo "Usage: wt-memory todo <add|list|done|clear>" >&2
            echo "  add [--tags t1,t2] < text    Save a todo (reads from stdin)" >&2
            echo "  list [--json]                 List open todos" >&2
            echo "  done <id>                     Mark todo as done (deletes it)" >&2
            echo "  clear --confirm               Delete all todos" >&2
            return 1
            ;;
        *)
            echo "Error: Unknown todo subcommand '$subcmd'" >&2
            return 1
            ;;
    esac
}

# todo add — save a todo from stdin
cmd_todo_add() {
    local extra_tags=""

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --tags) extra_tags="$2"; shift 2 ;;
            *) shift ;;
        esac
    done

    # Read content from stdin
    local content=""
    if [[ ! -t 0 ]]; then
        content=$(cat)
    fi

    if [[ -z "$content" ]]; then
        echo "Error: No todo text provided. Pipe content: echo \"text\" | wt-memory todo add" >&2
        return 1
    fi

    # Health check
    if ! cmd_health >/dev/null 2>&1; then
        echo "Memory system not available."
        return 0
    fi

    auto_migrate

    # Build tags: always include todo,backlog + optional extras + auto-detect change
    local tags="todo,backlog"
    if [[ -n "$extra_tags" ]]; then
        tags="$tags,$extra_tags"
    fi

    # Auto-detect active OpenSpec change
    local change_name=""
    if command -v openspec >/dev/null 2>&1; then
        change_name=$(openspec list --json 2>/dev/null | jq -r '[.changes[] | select(.status == "in-progress")] | .[0].name // empty' 2>/dev/null) || true
    fi
    if [[ -n "$change_name" ]]; then
        tags="$tags,change:$change_name"
    fi

    # Save using remember with metadata
    local storage_path
    storage_path=$(get_storage_path)
    local branch
    branch=$(get_current_branch)

    local tags_py
    tags_py=$(echo "$tags" | tr ',' '\n' | jq -R . | jq -s .)

    _SHODH_STORAGE="$storage_path" \
    _SHODH_CONTENT="$content" \
    _SHODH_TAGS="$tags_py" \
    _SHODH_BRANCH="$branch" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
tags = json.loads(os.environ['_SHODH_TAGS'])
branch = os.environ.get('_SHODH_BRANCH', '')
if branch:
    tags.append('branch:' + branch)
m.remember(
    os.environ['_SHODH_CONTENT'],
    memory_type='Context',
    tags=tags,
    metadata={'todo_status': 'open'}
)
print('Todo saved: \"' + os.environ['_SHODH_CONTENT'][:80] + '\"')
" || echo "Error: failed to save todo" >&2

    return 0
}

# todo list — show open todos
cmd_todo_list() {
    local json_mode=false

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --json) json_mode=true; shift ;;
            *) shift ;;
        esac
    done

    if ! cmd_health >/dev/null 2>&1; then
        if [[ "$json_mode" == "true" ]]; then
            echo "[]"
        else
            echo "No open todos."
        fi
        return 0
    fi

    auto_migrate

    local storage_path
    storage_path=$(get_storage_path)

    _SHODH_STORAGE="$storage_path" \
    _SHODH_JSON="$json_mode" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
json_mode = os.environ.get('_SHODH_JSON', 'false') == 'true'

# Get all memories tagged with 'todo'
if hasattr(m, 'recall_by_tags'):
    results = m.recall_by_tags(['todo'], limit=100)
else:
    results = m.recall('todo', limit=100, tags=['todo'])

# Filter to open todos only
todos = []
for r in results:
    meta = r.get('metadata', {}) or {}
    status = meta.get('todo_status', 'open')
    if status == 'open':
        todos.append(r)

if json_mode:
    print(json.dumps(todos, default=str))
else:
    if not todos:
        print('No open todos.')
    else:
        for t in todos:
            tid = t.get('id', '???')[:8]
            content = t.get('content', t.get('description', ''))
            ts = str(t.get('timestamp', t.get('created_at', '')))[:10]
            tags = [tg for tg in t.get('tags', []) if tg not in ('todo', 'backlog')]
            tag_str = ' [' + ', '.join(tags) + ']' if tags else ''
            print(f'  {tid}  {content[:72]}{tag_str}  ({ts})')
        print(f'\n{len(todos)} open todo(s)')
" || {
        if [[ "$json_mode" == "true" ]]; then
            echo "[]"
        else
            echo "No open todos."
        fi
    }

    return 0
}

# todo done — delete a todo by ID (prefix match)
cmd_todo_done() {
    local todo_id="${1:-}"

    if [[ -z "$todo_id" ]]; then
        echo "Error: specify a todo ID (use 'wt-memory todo list' to see IDs)" >&2
        return 1
    fi

    if ! cmd_health >/dev/null 2>&1; then
        echo "Memory system not available."
        return 0
    fi

    auto_migrate

    local storage_path
    storage_path=$(get_storage_path)

    _SHODH_STORAGE="$storage_path" \
    _SHODH_ID="$todo_id" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
prefix = os.environ['_SHODH_ID']

# Get all todos to find prefix match
if hasattr(m, 'recall_by_tags'):
    results = m.recall_by_tags(['todo'], limit=200)
else:
    results = m.recall('todo', limit=200, tags=['todo'])

# Find matching todo by ID prefix
matches = [r for r in results if r.get('id', '').startswith(prefix)]

if len(matches) == 0:
    print(f'Todo not found: {prefix}', file=sys.stderr)
    sys.exit(1)
elif len(matches) > 1:
    print(f'Ambiguous ID prefix \"{prefix}\" matches {len(matches)} todos. Use a longer prefix.', file=sys.stderr)
    sys.exit(1)

todo = matches[0]
full_id = todo['id']
content = todo.get('content', todo.get('description', ''))
m.forget(full_id)
print(f'Todo done: \"{content[:80]}\"')
" || { echo "Error: failed to complete todo" >&2; return 1; }

    return 0
}

# todo clear — delete all todos
cmd_todo_clear() {
    local confirm=false

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --confirm) confirm=true; shift ;;
            *) shift ;;
        esac
    done

    if [[ "$confirm" != "true" ]]; then
        echo "Error: Use --confirm to clear all todos" >&2
        return 1
    fi

    if ! cmd_health >/dev/null 2>&1; then
        echo "Memory system not available."
        return 0
    fi

    auto_migrate

    local storage_path
    storage_path=$(get_storage_path)

    _SHODH_STORAGE="$storage_path" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
count = m.forget_by_tags(['todo'])
print(f'Cleared {count} todos.')
" || echo "Error: failed to clear todos" >&2

    return 0
}

# Verify index integrity — find orphaned memories not in vector index
# Usage: wt-memory verify
cmd_verify() {
    if ! cmd_health >/dev/null 2>&1; then
        echo "{}"
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)
    mkdir -p "$storage_path"

    _SHODH_STORAGE="$storage_path" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
try:
    result = m.verify_index()
    print(json.dumps(result, default=str))
    if not result.get('is_healthy', True):
        import sys as _sys
        print('Hint: run \"wt-memory repair\" to fix orphaned memories', file=_sys.stderr)
except AttributeError:
    print(json.dumps({'error': 'verify_index not available — upgrade shodh-memory to >=0.1.81'}))
" || echo "{}"

    return 0
}

# Show consolidation report — memory strengthening/decay events
# Usage: wt-memory consolidation [--since ISO] [--events]
cmd_consolidation() {
    if ! cmd_health >/dev/null 2>&1; then
        echo "{}"
        return 0
    fi

    local since=""
    local events_only=false

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --since) since="$2"; shift 2 ;;
            --events) events_only=true; shift ;;
            *) shift ;;
        esac
    done

    local storage_path
    storage_path=$(get_storage_path)

    _SHODH_STORAGE="$storage_path" \
    _SHODH_SINCE="$since" \
    _SHODH_EVENTS="$events_only" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
since = os.environ.get('_SHODH_SINCE', '') or None
events_only = os.environ.get('_SHODH_EVENTS', 'false') == 'true'
try:
    if events_only:
        result = m.consolidation_events(since=since) if since else m.consolidation_events()
    else:
        if since:
            result = m.consolidation_report(since=since)
        else:
            result = m.consolidation_report()
    print(json.dumps(result, default=str))
except AttributeError:
    print(json.dumps({'error': 'consolidation not available in this shodh-memory version'}))
" || echo "{}"

    return 0
}

# Knowledge graph statistics
# Usage: wt-memory graph-stats
cmd_graph_stats() {
    if ! cmd_health >/dev/null 2>&1; then
        echo "{}"
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)

    _SHODH_STORAGE="$storage_path" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
try:
    result = m.graph_stats()
    print(json.dumps(result, default=str))
except AttributeError:
    print(json.dumps({'error': 'graph_stats not available in this shodh-memory version'}))
" || echo "{}"

    return 0
}

# Flush pending writes to disk
# Usage: wt-memory flush
cmd_flush() {
    if ! cmd_health >/dev/null 2>&1; then
        echo '{"flushed": false}'
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)

    _SHODH_STORAGE="$storage_path" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
m.flush()
print(json.dumps({'flushed': True}))
" || echo '{"flushed": false}'

    return 0
}

# Show configuration and health status
cmd_status() {
    local json_mode=false

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --json) json_mode=true; shift ;;
            *) shift ;;
        esac
    done

    local project
    project=$(resolve_project)
    local storage_path
    storage_path=$(get_storage_path)

    if [[ "$json_mode" == "true" ]]; then
        # JSON output for GUI consumption
        if cmd_health >/dev/null 2>&1; then
            local count
            count=$(_SHODH_STORAGE="$storage_path" run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
stats = m.get_stats()
print(stats.get('total_memories', 0))
") || count=0
            [[ "$count" =~ ^[0-9]+$ ]] || count=0
            echo "{\"available\": true, \"project\": \"$project\", \"count\": $count, \"storage_path\": \"$storage_path\"}"
        else
            echo "{\"available\": false, \"project\": \"$project\", \"count\": 0, \"storage_path\": \"$storage_path\"}"
        fi
    else
        # Human-readable output
        echo "Shodh-Memory Configuration:"
        echo "  Project: $project"
        echo "  Storage: $storage_path"
        echo ""

        echo -n "Health: "
        if cmd_health >/dev/null 2>&1; then
            echo "available"
            _SHODH_STORAGE="$storage_path" run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
stats = m.get_stats()
print(f'  Memories: {stats.get(\"total_memories\", \"?\")}')" || true
        else
            echo "not installed"
            echo ""
            echo "Install:"
            echo "  pip install shodh-memory"
        fi
    fi
}

# List all projects with memory counts
cmd_projects() {
    if [[ ! -d "$SHODH_STORAGE" ]]; then
        echo "No memory storage found at $SHODH_STORAGE"
        return 0
    fi

    local has_projects=false

    for dir in "$SHODH_STORAGE"/*/; do
        [[ -d "$dir" ]] || continue
        local proj_name
        proj_name=$(basename "$dir")

        # Skip if it's not a shodh-memory storage (no memories subdir)
        [[ -d "$dir/memories" ]] || [[ -d "$dir/memory_index" ]] || continue

        has_projects=true

        if cmd_health >/dev/null 2>&1; then
            local count
            count=$(_SHODH_STORAGE="$dir" run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
stats = m.get_stats()
print(stats.get('total_memories', 0))
") || count="?"
            echo "  $proj_name: $count memories"
        else
            echo "  $proj_name: (shodh-memory not installed)"
        fi
    done

    # Check for legacy storage (sst files directly in root)
    if ls "$SHODH_STORAGE"/*.sst >/dev/null 2>&1; then
        has_projects=true
        echo "  _legacy: (unmigrared global storage)"
    fi

    if [[ "$has_projects" == "false" ]]; then
        echo "No projects with memories found."
    fi
}

# --- Migrations: versioned memory storage transformations ---

# Read the .migrations state file. Returns JSON or '{"applied":[]}' if missing.
migrations_read() {
    local storage_path="$1"
    local mig_file="$storage_path/.migrations"
    if [[ -f "$mig_file" ]]; then
        cat "$mig_file"
    else
        echo '{"applied":[]}'
    fi
}

# Write the .migrations state file atomically.
migrations_write() {
    local storage_path="$1"
    local data="$2"
    local mig_file="$storage_path/.migrations"
    local tmp_file="$mig_file.tmp.$$"
    echo "$data" > "$tmp_file"
    mv "$tmp_file" "$mig_file"
}

# Check if a migration ID has been applied.
migration_is_applied() {
    local storage_path="$1"
    local migration_id="$2"
    local state
    state=$(migrations_read "$storage_path")
    echo "$state" | jq -e --arg id "$migration_id" '.applied | index($id) != null' >/dev/null 2>&1
}

# Mark a migration as applied.
migration_mark_applied() {
    local storage_path="$1"
    local migration_id="$2"
    local state
    state=$(migrations_read "$storage_path")
    local new_state
    new_state=$(echo "$state" | jq --arg id "$migration_id" '.applied += [$id] | .last_run = now | .last_run = (now | todate)')
    migrations_write "$storage_path" "$new_state"
}

# Migration 001: Add branch:unknown tag to memories without any branch:* tag
migrate_001_branch_tags() {
    local storage_path="$1"

    if ! cmd_health >/dev/null 2>&1; then
        return 0
    fi

    _SHODH_STORAGE="$storage_path" \
    run_with_lock run_shodh_python -c "
import sys; sys._shodh_star_shown = True
import json, os
from shodh_memory import Memory
m = Memory(storage_path=os.environ['_SHODH_STORAGE'])
memories = m.list_memories()
updated = 0
for mem in memories:
    tags = mem.get('tags', [])
    has_branch = any(t.startswith('branch:') for t in tags)
    if not has_branch:
        new_tags = tags + ['branch:unknown']
        try:
            # shodh-memory has no update_memory — delete and re-create
            m.forget(mem['id'])
            m.remember(
                mem.get('content', ''),
                memory_type=mem.get('experience_type', 'Context'),
                tags=new_tags,
                entities=mem.get('entities', []),
                metadata=mem.get('metadata', {}),
                is_failure=mem.get('is_failure', False),
                is_anomaly=mem.get('is_anomaly', False),
            )
            updated += 1
        except Exception as e:
            print(f'Migration warning: {e}', file=sys.stderr)
print(f'{updated} memories tagged with branch:unknown', file=sys.stderr)
" || true
}

# List of all migrations in order. Format: ID:function_name:description
MIGRATIONS=(
    "001:migrate_001_branch_tags:Add branch:unknown tag to pre-existing memories"
)

# Run all pending migrations.
# Returns 0 on success, outputs progress to stderr.
run_migrations() {
    local storage_path="$1"
    local verbose="${2:-false}"
    mkdir -p "$storage_path"

    local count=0
    for entry in "${MIGRATIONS[@]}"; do
        local id func desc
        id="${entry%%:*}"
        local rest="${entry#*:}"
        func="${rest%%:*}"
        desc="${rest#*:}"

        if migration_is_applied "$storage_path" "$id"; then
            [[ "$verbose" == "true" ]] && echo "$id: $desc — already applied"
            continue
        fi

        [[ "$verbose" == "true" ]] && echo -n "$id: $desc — "
        "$func" "$storage_path"
        migration_mark_applied "$storage_path" "$id"
        [[ "$verbose" == "true" ]] && echo "applied"
        count=$((count + 1))
    done

    if [[ $count -gt 0 ]]; then
        echo "Migrating memory storage... done ($count migration(s) applied)" >&2
    fi
}

# Auto-migrate: run pending migrations before storage access.
# Skipped if --no-migrate is in the global args or storage doesn't exist yet.
AUTO_MIGRATE_DONE=false
NO_MIGRATE=false

auto_migrate() {
    if [[ "$AUTO_MIGRATE_DONE" == "true" ]] || [[ "$NO_MIGRATE" == "true" ]]; then
        return 0
    fi
    AUTO_MIGRATE_DONE=true

    if [[ -z "$SHODH_PYTHON" ]]; then
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)

    # Only run if storage directory exists (don't create it just for migration)
    if [[ ! -d "$storage_path" ]]; then
        return 0
    fi

    run_migrations "$storage_path" false
}

# Manual migrate subcommand
# Usage: wt-memory migrate [--status]
cmd_migrate() {
    local show_status=false

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --status) show_status=true; shift ;;
            *) shift ;;
        esac
    done

    if [[ -z "$SHODH_PYTHON" ]]; then
        return 0
    fi

    local storage_path
    storage_path=$(get_storage_path)
    mkdir -p "$storage_path"

    if [[ "$show_status" == "true" ]]; then
        local state
        state=$(migrations_read "$storage_path")
        local applied
        applied=$(echo "$state" | jq -r '.applied[]' 2>/dev/null || true)

        for entry in "${MIGRATIONS[@]}"; do
            local id func desc
            id="${entry%%:*}"
            local rest="${entry#*:}"
            func="${rest%%:*}"
            desc="${rest#*:}"

            if echo "$applied" | grep -q "^${id}$"; then
                local ts
                ts=$(echo "$state" | jq -r '.last_run // "unknown"' 2>/dev/null)
                echo "$id: $desc — applied ($ts)"
            else
                echo "$id: $desc — pending"
            fi
        done
        return 0
    fi

    # Run all pending migrations (verbose)
    local count=0
    for entry in "${MIGRATIONS[@]}"; do
        local id func desc
        id="${entry%%:*}"
        local rest="${entry#*:}"
        func="${rest%%:*}"
        desc="${rest#*:}"

        if migration_is_applied "$storage_path" "$id"; then
            echo "$id: $desc — already applied"
            continue
        fi

        echo -n "$id: $desc — "
        "$func" "$storage_path"
        migration_mark_applied "$storage_path" "$id"
        echo "applied"
        count=$((count + 1))
    done

    if [[ $count -eq 0 ]]; then
        echo "All migrations applied."
    fi
}

# --- Sync: Git-based memory sharing via orphan branch ---

# Resolve sync identity: <user>/<machine>
# User from git config user.name (lowercase, sanitized), fallback: whoami
# Machine from hostname -s (lowercase, sanitized)
sync_resolve_identity() {
    local user machine

    user=$(git config user.name 2>/dev/null || true)
    if [[ -z "$user" ]]; then
        user=$(whoami 2>/dev/null || echo "unknown")
    fi
    user=$(echo "$user" | tr '[:upper:]' '[:lower:]' | tr ' ' '-' | tr -cd 'a-z0-9-')

    machine=$(hostname -s 2>/dev/null || hostname 2>/dev/null || echo "unknown")
    machine=$(echo "$machine" | tr '[:upper:]' '[:lower:]' | tr -cd 'a-z0-9-')

    echo "${user}/${machine}"
}

# Read a key from .sync-state JSON
sync_get_state() {
    local storage_path="$1"
    local key="$2"
    local state_file="$storage_path/.sync-state"

    if [[ -f "$state_file" ]]; then
        jq -r ".[\"$key\"] // empty" "$state_file" 2>/dev/null || true
    fi
}

# Update keys in .sync-state JSON (key=value pairs)
sync_update_state() {
    local storage_path="$1"
    shift
    local state_file="$storage_path/.sync-state"

    local current='{}'
    if [[ -f "$state_file" ]]; then
        current=$(cat "$state_file")
    fi

    while [[ $# -gt 0 ]]; do
        local kv="$1"
        local key="${kv%%=*}"
        local val="${kv#*=}"
        current=$(echo "$current" | jq --arg k "$key" --arg v "$val" '. + {($k): $v}')
        shift
    done

    echo "$current" > "$state_file"
}

# Check sync preconditions. Returns 0 if ready, 1 if should skip, 2 if error.
sync_check_preconditions() {
    # Check shodh-memory
    if [[ -z "$SHODH_PYTHON" ]]; then
        return 1  # silent skip
    fi

    # Check git repo
    if ! git rev-parse --git-dir >/dev/null 2>&1; then
        echo "Error: not a git repository" >&2
        return 2
    fi

    # Check remote
    if ! git remote get-url origin >/dev/null 2>&1; then
        echo "Error: no git remote 'origin' found" >&2
        return 2
    fi

    return 0
}

# Push memory to git remote
cmd_sync_push() {
    local rc=0
    sync_check_preconditions || rc=$?
    if [[ $rc -eq 1 ]]; then return 0; fi
    if [[ $rc -eq 2 ]]; then return 1; fi

    local storage_path
    storage_path=$(get_storage_path)
    mkdir -p "$storage_path"

    local identity
    identity=$(sync_resolve_identity)

    # Export memory
    local export_json
    export_json=$(cmd_export) || { echo "Error: export failed" >&2; return 1; }

    # Hash check — skip if nothing changed since last push
    # Normalize: strip volatile fields (exported_at, last_accessed, access_count, importance)
    # so that only content changes trigger a push
    local current_hash
    current_hash=$(echo "$export_json" | jq -S '[.records[] | {id, content, experience_type, tags, metadata}] | sort_by(.id)' | sha256sum | cut -d' ' -f1)
    local last_hash
    last_hash=$(sync_get_state "$storage_path" "last_push_hash")

    if [[ -n "$last_hash" && "$current_hash" == "$last_hash" ]]; then
        echo "Nothing to push."
        return 0
    fi

    local remote_url
    remote_url=$(git remote get-url origin)

    # Get user identity for commits in temp dir
    local git_name git_email
    git_name=$(git config user.name 2>/dev/null || echo "wt-memory")
    git_email=$(git config user.email 2>/dev/null || echo "wt-memory@localhost")

    local tmpdir
    tmpdir=$(mktemp -d)

    # Check if wt-memory branch exists on remote
    local branch_exists=false
    if git ls-remote --heads origin wt-memory 2>/dev/null | grep -q wt-memory; then
        branch_exists=true
    fi

    if [[ "$branch_exists" == "true" ]]; then
        # Clone existing branch (shallow)
        if ! git clone --branch wt-memory --single-branch --depth 1 "$remote_url" "$tmpdir" 2>/dev/null; then
            echo "Error: failed to clone wt-memory branch" >&2
            rm -rf "$tmpdir"
            return 1
        fi
    else
        # Create new orphan branch
        git init "$tmpdir" >/dev/null 2>&1
        git -C "$tmpdir" checkout --orphan wt-memory 2>/dev/null
        git -C "$tmpdir" remote add origin "$remote_url" 2>/dev/null
    fi

    # Configure git in temp dir
    git -C "$tmpdir" config user.name "$git_name"
    git -C "$tmpdir" config user.email "$git_email"

    # Write export file
    mkdir -p "$tmpdir/$identity"
    echo "$export_json" > "$tmpdir/$identity/memories.json"

    # Stage changes
    git -C "$tmpdir" add -A

    # Check if there are actual changes to commit
    if git -C "$tmpdir" diff --cached --quiet 2>/dev/null; then
        echo "Nothing to push."
        rm -rf "$tmpdir"
        # Update hash even though nothing changed on remote — content matches
        sync_update_state "$storage_path" \
            "last_push_hash=$current_hash" \
            "last_push_at=$(date -u +%Y-%m-%dT%H:%M:%SZ)"
        return 0
    fi

    local timestamp
    timestamp=$(date -Iseconds 2>/dev/null || date)
    git -C "$tmpdir" commit -m "sync: $identity $timestamp" >/dev/null 2>&1

    if git -C "$tmpdir" push origin wt-memory 2>/dev/null; then
        sync_update_state "$storage_path" \
            "last_push_hash=$current_hash" \
            "last_push_at=$(date -u +%Y-%m-%dT%H:%M:%SZ)"
        echo "Pushed to $identity"
    else
        echo "Error: push failed (remote may have changed, try again)" >&2
        rm -rf "$tmpdir"
        return 1
    fi

    rm -rf "$tmpdir"
}

# Pull memory from git remote
cmd_sync_pull() {
    local from_filter=""

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --from) from_filter="$2"; shift 2 ;;
            *) shift ;;
        esac
    done

    local rc=0
    sync_check_preconditions || rc=$?
    if [[ $rc -eq 1 ]]; then return 0; fi
    if [[ $rc -eq 2 ]]; then return 1; fi

    local storage_path
    storage_path=$(get_storage_path)
    mkdir -p "$storage_path"

    local identity
    identity=$(sync_resolve_identity)

    # Fetch the wt-memory branch
    if ! git fetch origin wt-memory 2>/dev/null; then
        echo "No sync branch found. Run 'wt-memory sync push' first."
        return 0
    fi

    # Check if remote changed since last pull
    local remote_commit
    remote_commit=$(git rev-parse FETCH_HEAD 2>/dev/null)
    local last_commit
    last_commit=$(sync_get_state "$storage_path" "last_pull_commit")

    if [[ -n "$last_commit" && "$remote_commit" == "$last_commit" && -z "$from_filter" ]]; then
        echo "Up to date."
        return 0
    fi

    # List all memory files on the branch
    local files
    files=$(git ls-tree -r --name-only FETCH_HEAD 2>/dev/null | grep '/memories\.json$' || true)

    if [[ -z "$files" ]]; then
        echo "No memory files found on sync branch."
        sync_update_state "$storage_path" \
            "last_pull_commit=$remote_commit" \
            "last_pull_at=$(date -u +%Y-%m-%dT%H:%M:%SZ)"
        return 0
    fi

    local tmpdir
    tmpdir=$(mktemp -d)

    local total_imported=0
    local total_skipped=0
    local sources_processed=0

    while IFS= read -r file_path; do
        # Extract identity from path: <user>/<machine>/memories.json
        local source_identity
        source_identity=$(dirname "$file_path")

        # Skip own files
        if [[ "$source_identity" == "$identity" ]]; then
            continue
        fi

        # Apply --from filter
        if [[ -n "$from_filter" && "$source_identity" != "$from_filter" ]]; then
            continue
        fi

        # Extract file content via git show
        local tmp_file="$tmpdir/$(echo "$source_identity" | tr '/' '-').json"
        if ! git show "FETCH_HEAD:$file_path" > "$tmp_file" 2>/dev/null; then
            continue
        fi

        # Import using existing cmd_import (captures JSON result)
        local result
        result=$(cmd_import "$tmp_file" 2>/dev/null) || continue

        local imported skipped
        imported=$(echo "$result" | jq -r '.imported // 0' 2>/dev/null) || imported=0
        skipped=$(echo "$result" | jq -r '.skipped // 0' 2>/dev/null) || skipped=0

        echo "$source_identity: $imported new, $skipped skipped"
        total_imported=$((total_imported + imported))
        total_skipped=$((total_skipped + skipped))
        sources_processed=$((sources_processed + 1))
    done <<< "$files"

    rm -rf "$tmpdir"

    if [[ $sources_processed -eq 0 ]]; then
        echo "No other sources found."
    else
        echo "Total: $total_imported new, $total_skipped skipped from $sources_processed source(s)"
    fi

    sync_update_state "$storage_path" \
        "last_pull_commit=$remote_commit" \
        "last_pull_at=$(date -u +%Y-%m-%dT%H:%M:%SZ)"
}

# Sync dispatch: push, pull, status, or push+pull
cmd_sync() {
    if [[ $# -gt 0 ]]; then
        case "$1" in
            push)   shift; cmd_sync_push "$@"; return $? ;;
            pull)   shift; cmd_sync_pull "$@"; return $? ;;
            status) shift; cmd_sync_status "$@"; return $? ;;
            -*)     shift ;; # ignore flags for bare sync
            *)
                echo "Error: unknown sync subcommand '$1'" >&2
                echo "Usage: wt-memory sync [push|pull|status]" >&2
                return 1
                ;;
        esac
    fi

    # No subcommand: push + pull
    echo "--- Push ---"
    cmd_sync_push || true
    echo ""
    echo "--- Pull ---"
    cmd_sync_pull
}

# Show sync status
cmd_sync_status() {
    local rc=0
    sync_check_preconditions || rc=$?
    if [[ $rc -eq 1 ]]; then echo "shodh-memory not installed"; return 0; fi
    if [[ $rc -eq 2 ]]; then return 1; fi

    local storage_path
    storage_path=$(get_storage_path)
    local identity
    identity=$(sync_resolve_identity)

    echo "Identity: $identity"
    echo ""

    # Show local state
    local last_push_at last_pull_at
    last_push_at=$(sync_get_state "$storage_path" "last_push_at")
    last_pull_at=$(sync_get_state "$storage_path" "last_pull_at")

    if [[ -n "$last_push_at" || -n "$last_pull_at" ]]; then
        echo "Last sync:"
        [[ -n "$last_push_at" ]] && echo "  Push: $last_push_at"
        [[ -n "$last_pull_at" ]] && echo "  Pull: $last_pull_at"
    else
        echo "Never synced."
    fi
    echo ""

    # Show remote sources
    echo "Remote sources:"
    if git fetch origin wt-memory 2>/dev/null; then
        local files
        files=$(git ls-tree -r --name-only FETCH_HEAD 2>/dev/null | grep '/memories\.json$' || true)

        if [[ -n "$files" ]]; then
            while IFS= read -r file_path; do
                local source
                source=$(dirname "$file_path")
                if [[ "$source" == "$identity" ]]; then
                    echo "  $source (you)"
                else
                    echo "  $source"
                fi
            done <<< "$files"
        else
            echo "  (none)"
        fi
    else
        echo "  (no sync branch)"
    fi
}

# ============================================================
# Metrics & Reporting
# ============================================================

cmd_metrics() {
    local since_days=7
    local json_output=false

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --enable)
                local py
                py=$(find_python)
                "$py" -c "
import sys
sys.path.insert(0, '$_wt_memory_bin_dir/..')
from lib.metrics import enable
enable()
print('Metrics collection enabled.')
print('Injection data will be recorded in session caches and flushed to SQLite on session end.')
"
                return 0
                ;;
            --disable)
                local py
                py=$(find_python)
                "$py" -c "
import sys
sys.path.insert(0, '$_wt_memory_bin_dir/..')
from lib.metrics import disable
disable()
print('Metrics collection disabled.')
"
                return 0
                ;;
            --since)
                shift
                since_days="${1%d}"  # strip trailing 'd' if present
                shift
                ;;
            --json)
                json_output=true
                shift
                ;;
            *)
                echo "Unknown option: $1" >&2
                return 1
                ;;
        esac
    done

    local py
    py=$(find_python)
    "$py" -c "
import sys, json
sys.path.insert(0, '$_wt_memory_bin_dir/..')
from lib.metrics import query_report, format_tui_report, is_enabled

data = query_report(since_days=$since_days)

if data is None:
    if not is_enabled():
        print('Metrics collection is disabled.')
        print('Enable with: wt-memory metrics --enable')
    else:
        print('No metrics data yet. Data is collected after sessions end.')
    sys.exit(0)

if $([[ "$json_output" == "true" ]] && echo "True" || echo "False"):
    print(json.dumps(data, indent=2))
else:
    print(format_tui_report(data))
"
}

cmd_dashboard() {
    local since_days=30

    while [[ $# -gt 0 ]]; do
        case "$1" in
            --since)
                shift
                since_days="${1%d}"
                shift
                ;;
            *)
                echo "Unknown option: $1" >&2
                return 1
                ;;
        esac
    done

    local py
    py=$(find_python)
    local html_file="/tmp/wt-memory-dashboard.html"

    "$py" -c "
import sys, json, os
sys.path.insert(0, '$_wt_memory_bin_dir/..')
from lib.metrics import query_report, query_session_injections, is_enabled
from lib.dashboard import generate_dashboard

data = query_report(since_days=$since_days)
if data is None:
    if not is_enabled():
        print('Metrics collection is disabled. Enable with: wt-memory metrics --enable')
    else:
        print('No metrics data yet.')
    sys.exit(0)

# Enrich with per-session injection details for drill-down
for session in data.get('sessions', []):
    session['injections'] = query_session_injections(session['id'])

html = generate_dashboard(data)
with open('$html_file', 'w') as f:
    f.write(html)
print(f'Dashboard written to: $html_file')
"

    # Open in browser
    if [[ -f "$html_file" ]]; then
        if command -v xdg-open &>/dev/null; then
            xdg-open "$html_file" 2>/dev/null &
        elif command -v open &>/dev/null; then
            open "$html_file" 2>/dev/null &
        else
            echo "Open in browser: $html_file"
        fi
    fi
}

# Main dispatch — parse global --project flag before command
main() {
    # Parse global options before command
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --project)
                PROJECT="$2"
                shift 2
                ;;
            --no-migrate)
                NO_MIGRATE=true
                shift
                ;;
            *)
                break
                ;;
        esac
    done

    if [[ $# -eq 0 ]]; then
        usage
        exit 0
    fi

    local command="$1"
    shift

    case "$command" in
        health)   cmd_health "$@" ;;
        remember) cmd_remember "$@" ;;
        recall)   cmd_recall "$@" ;;
        proactive) cmd_proactive "$@" ;;
        list)     cmd_list "$@" ;;
        forget)   cmd_forget "$@" ;;
        context)  cmd_context "$@" ;;
        brain)    cmd_brain "$@" ;;
        stats)    cmd_stats "$@" ;;
        cleanup)  cmd_cleanup "$@" ;;
        audit)    cmd_audit "$@" ;;
        dedup)    cmd_dedup "$@" ;;
        get|inspect) cmd_get "$@" ;;
        export)   cmd_export "$@" ;;
        import)   cmd_import "$@" ;;
        sync)     cmd_sync "$@" ;;
        migrate)  cmd_migrate "$@" ;;
        repair)   cmd_repair "$@" ;;
        verify)   cmd_verify "$@" ;;
        consolidation) cmd_consolidation "$@" ;;
        graph-stats) cmd_graph_stats "$@" ;;
        flush)    cmd_flush "$@" ;;
        todo)     cmd_todo "$@" ;;
        rules)    cmd_rules "$@" ;;
        status)   cmd_status "$@" ;;
        metrics)  cmd_metrics "$@" ;;
        dashboard) cmd_dashboard "$@" ;;
        projects) cmd_projects "$@" ;;
        -h|--help|help) usage ;;
        *)
            echo "Error: Unknown command '$command'" >&2
            usage
            exit 1
            ;;
    esac
}

main "$@"
