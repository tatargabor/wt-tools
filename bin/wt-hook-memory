#!/usr/bin/env bash
# wt-hook-memory - Unified Claude Code memory hook handler
#
# Single handler replacing 5 separate scripts. Dispatches by event type.
# Usage: wt-hook-memory <EventName>
#
# Events: SessionStart, UserPromptSubmit, PreToolUse, PostToolUse,
#         PostToolUseFailure, SubagentStop, Stop
#
# Configuration in .claude/settings.json via wt-deploy-hooks.

set -e

EVENT="$1"
[[ -z "$EVENT" ]] && exit 0

# --- Resolve wt-tools root (follow symlinks) ---
_resolve_wt_root() {
    local script="${BASH_SOURCE[0]}"
    while [[ -L "$script" ]]; do
        local link
        link=$(readlink "$script")
        [[ "$link" == /* ]] && script="$link" || script="$(dirname "$script")/$link"
    done
    dirname "$(cd "$(dirname "$script")" && pwd)"
}
WT_TOOLS_ROOT="$(_resolve_wt_root)"

# --- Logging ---
DEBUG_LOG="/tmp/wt-hook-memory.log"
[[ -f /tmp/wt-hook-memory.debug ]] && WT_HOOK_DEBUG=1

# Always-on lightweight log: one line per hook invocation (for prod debugging)
_log() {
    echo "[$(date '+%H:%M:%S')] [$EVENT] $*" >> "$DEBUG_LOG"
}

# Verbose debug log: gated on WT_HOOK_DEBUG=1
_dbg() {
    [[ "${WT_HOOK_DEBUG:-}" == "1" ]] || return 0
    echo "[$(date '+%H:%M:%S')] [$EVENT] DBG $*" >> "$DEBUG_LOG"
}

_dbg "=== START ==="

# --- Metrics collection ---
METRICS_ENABLED_FLAG="${HOME}/.local/share/wt-tools/metrics/.enabled"
METRICS_ENABLED=0
[[ -f "$METRICS_ENABLED_FLAG" ]] && METRICS_ENABLED=1
_METRICS_TIMER_START=0

_metrics_timer_start() {
    [[ "$METRICS_ENABLED" -eq 0 ]] && return 0
    _METRICS_TIMER_START=$(date +%s%3N 2>/dev/null || python3 -c "import time; print(int(time.time()*1000))")
}

_metrics_timer_elapsed() {
    [[ "$METRICS_ENABLED" -eq 0 ]] && { echo 0; return 0; }
    local now
    now=$(date +%s%3N 2>/dev/null || python3 -c "import time; print(int(time.time()*1000))")
    echo $(( now - _METRICS_TIMER_START ))
}

# Append a metrics record to session cache _metrics array.
# Args: layer event query result_count filtered_count relevance_scores_json duration_ms token_estimate dedup_hit
_metrics_append() {
    [[ "$METRICS_ENABLED" -eq 0 ]] && return 0
    local layer="$1" event="$2" query="${3:0:500}" result_count="${4:-0}" filtered_count="${5:-0}"
    local scores_json="${6:-[]}" duration_ms="${7:-0}" token_estimate="${8:-0}" dedup_hit="${9:-0}"

    python3 -c "
import json, sys, os
from datetime import datetime, timezone

cache_file = sys.argv[1]
cache = {}
if os.path.exists(cache_file):
    try:
        with open(cache_file) as f:
            cache = json.load(f)
    except: pass

metrics = cache.get('_metrics', [])
if len(metrics) >= 500:
    sys.exit(0)

scores = json.loads(sys.argv[8])
avg_r = sum(scores)/len(scores) if scores else None
max_r = max(scores) if scores else None
min_r = min(scores) if scores else None

metrics.append({
    'ts': datetime.now(timezone.utc).isoformat(),
    'layer': sys.argv[2],
    'event': sys.argv[3],
    'query': sys.argv[4],
    'result_count': int(sys.argv[5]),
    'filtered_count': int(sys.argv[6]),
    'avg_relevance': round(avg_r, 4) if avg_r is not None else None,
    'max_relevance': round(max_r, 4) if max_r is not None else None,
    'min_relevance': round(min_r, 4) if min_r is not None else None,
    'duration_ms': int(sys.argv[7]),
    'token_estimate': int(sys.argv[9]),
    'dedup_hit': int(sys.argv[10]),
})
cache['_metrics'] = metrics
with open(cache_file, 'w') as f:
    json.dump(cache, f)
" "$CACHE_FILE" "$layer" "$event" "$query" "$result_count" "$filtered_count" \
  "$duration_ms" "$scores_json" "$token_estimate" "$dedup_hit" 2>/dev/null || true
}

# Extract relevance scores from proactive/recall JSON output.
# Reads from $TMPFILE, outputs JSON array of floats.
_extract_scores() {
    python3 -c "
import json, sys
try:
    memories = json.load(open(sys.argv[1]))
except: memories = []
scores = []
for m in memories:
    s = m.get('relevance_score')
    if s is not None and s != 'N/A':
        try: scores.append(round(float(s), 4))
        except: pass
print(json.dumps(scores))
" "$TMPFILE" 2>/dev/null || echo "[]"
}

# --- Health check (single, shared) ---
if ! command -v wt-memory &>/dev/null; then
    _dbg "SKIP: wt-memory not in PATH"
    exit 0
fi
if ! wt-memory health &>/dev/null; then
    _dbg "SKIP: wt-memory unhealthy"
    exit 0
fi

# --- Store input in temp file (efficient for large PostToolUse payloads) ---
INPUT_FILE=$(mktemp)
TMPFILE=$(mktemp)
trap 'rm -f "$INPUT_FILE" "$TMPFILE"' EXIT
cat > "$INPUT_FILE"

_dbg "input: $(wc -c < "$INPUT_FILE") bytes"

# --- Session ID for dedup cache ---
SESSION_ID=$(python3 -c "import json; print(json.load(open('$INPUT_FILE')).get('session_id','unknown'))" 2>/dev/null || echo "unknown")
CACHE_FILE="/tmp/wt-memory-session-${SESSION_ID}.json"

_dbg "session=$SESSION_ID"

# ============================================================
# Helper: Session dedup cache
# ============================================================

dedup_clear() {
    _dbg "dedup_clear: removing $CACHE_FILE"
    rm -f "$CACHE_FILE"
}

dedup_check() {
    [[ ! -f "$CACHE_FILE" ]] && { _dbg "dedup_check: no cache file, miss"; return 1; }
    if python3 -c "
import json, sys
with open(sys.argv[1]) as f:
    cache = json.load(f)
sys.exit(0 if sys.argv[2] in cache else 1)
" "$CACHE_FILE" "$1" 2>/dev/null; then
        _dbg "dedup_check: HIT key=$1"
        return 0
    else
        _dbg "dedup_check: MISS key=$1"
        return 1
    fi
}

dedup_add() {
    _dbg "dedup_add: key=$1"
    python3 -c "
import json, sys, os
cache = {}
if os.path.exists(sys.argv[1]):
    try:
        with open(sys.argv[1]) as f:
            cache = json.load(f)
    except: pass
cache[sys.argv[2]] = 1
with open(sys.argv[1], 'w') as f:
    json.dump(cache, f)
" "$CACHE_FILE" "$1" 2>/dev/null
}

make_dedup_key() {
    echo -n "$1:$2:$3" | md5sum 2>/dev/null | cut -c1-16 || \
    echo -n "$1:$2:$3" | sha256sum 2>/dev/null | cut -c1-16
}

# ============================================================
# Helper: Load matching rules from .claude/rules.yaml
# Returns formatted MANDATORY RULES block, or empty string.
# Silently skips if file absent, malformed, or yaml unavailable.
# ============================================================

load_matching_rules() {
    local prompt_text="$1"

    local project_root
    project_root=$(git rev-parse --show-toplevel 2>/dev/null) || project_root="${CLAUDE_PROJECT_DIR:-$(pwd)}"
    local rules_file="${project_root}/.claude/rules.yaml"

    [[ -f "$rules_file" ]] || { _dbg "rules: no file"; return 0; }

    # Write python to a temp file â€” avoids heredoc-inside-$() bash parse issues
    local py_tmp
    py_tmp=$(mktemp /tmp/wt-rules.XXXXXX.py)
    # NOTE: no parens/braces inside the heredoc body that bash would misparse
    cat > "$py_tmp" <<'RULES_PY'
import sys
rules_file = sys.argv[1]
prompt_lower = sys.argv[2].lower()
try:
    import yaml
except ImportError:
    sys.exit(0)
try:
    data = yaml.safe_load(open(rules_file))
except Exception:
    sys.exit(0)
if not isinstance(data, dict):
    sys.exit(0)
rules = data.get("rules")
if not isinstance(rules, list):
    sys.exit(0)
matched = []
for rule in rules:
    if not isinstance(rule, dict):
        continue
    topics = rule.get("topics") or []
    content = (rule.get("content") or "").strip()
    rid = rule.get("id") or ""
    if not topics or not content:
        continue
    hit = False
    for t in topics:
        if str(t).lower() in prompt_lower:
            hit = True
            break
    if hit:
        matched.append(rid + "\t" + content)
if matched:
    print("=== MANDATORY RULES ===")
    for m in matched:
        rid, cnt = m.split("\t", 1)
        print("[" + rid + "] " + cnt)
    print("===========================")
RULES_PY

    local result
    result=$(python3 "$py_tmp" "$rules_file" "$prompt_text" 2>/dev/null) || true
    rm -f "$py_tmp"

    if [[ -n "$result" ]]; then
        local rule_count
        rule_count=$(printf '%s\n' "$result" | grep -c '^\[' 2>/dev/null || echo "?")
        _log "rules: injecting $rule_count matching rule(s)"
        _dbg "rules block: ${result:0:200}"
        printf '%s\n' "$result"
    else
        _dbg "rules: no matches"
    fi
}

# ============================================================
# Helper: Proactive recall with relevance filtering
# ============================================================

proactive_and_format() {
    local query="$1" limit="${2:-5}"
    _log "proactive: query='${query:0:80}' limit=$limit"

    wt-memory proactive "$query" --limit "$limit" 2>/dev/null > "$TMPFILE" || { _log "proactive: FAILED"; return 1; }

    local result
    result=$(python3 -c "
import sys, json
try:
    memories = json.load(open(sys.argv[1]))
except: sys.exit(1)
if not memories: sys.exit(1)
print(f'total={len(memories)}', file=sys.stderr)
filtered = []
for m in memories:
    score = m.get('relevance_score')
    if score is not None and score != 'N/A':
        try:
            if float(score) < 0.3:
                print(f'  skip [{score:.2f}]: {m.get(\"content\",\"\")[:60]}', file=sys.stderr)
                continue
        except (ValueError, TypeError): pass
    filtered.append(m)
if not filtered: sys.exit(1)
seen = set()
emitted = 0
for m in filtered:
    c = m.get('content','').replace('\n',' ').strip()
    if len(c) < 20:
        print(f'  skip [short]: {c!r}', file=sys.stderr)
        continue
    key = c[:50]
    if key in seen: continue
    seen.add(key)
    score = m.get('relevance_score', '?')
    try: score = f'{float(score):.2f}'
    except: pass
    print(f'  [{score}] {c[:100]}', file=sys.stderr)
    print(f'  - {c}')
    emitted += 1
if not emitted: sys.exit(1)
print(f'filtered={emitted}', file=sys.stderr)
" "$TMPFILE" 2>"$TMPFILE.err")
    local rc=$?
    if [[ -f "$TMPFILE.err" ]]; then
        while IFS= read -r line; do
            _log "proactive: $line"
        done < "$TMPFILE.err"
    fi
    rm -f "$TMPFILE.err"
    [[ $rc -ne 0 ]] && { _log "proactive: no results after filtering"; return 1; }
    echo "$result"
}

recall_and_format() {
    local query="$1" limit="${2:-3}" mode="${3:-hybrid}"
    _log "recall: query='${query:0:80}' mode=$mode limit=$limit"

    wt-memory recall "$query" --limit "$limit" --mode "$mode" 2>/dev/null > "$TMPFILE" || { _log "recall: FAILED"; return 1; }

    local result
    result=$(python3 -c "
import sys, json
try:
    memories = json.load(open(sys.argv[1]))
except: sys.exit(1)
if not memories: sys.exit(1)
print(f'total={len(memories)}', file=sys.stderr)
seen = set()
for m in memories:
    score = m.get('relevance_score')
    if score is not None and score != 'N/A':
        try:
            if float(score) < 0.3:
                print(f'  skip [{float(score):.2f}]: {m.get(\"content\",\"\")[:60]}', file=sys.stderr)
                continue
        except (ValueError, TypeError): pass
    c = m.get('content','').replace('\n',' ').strip()
    if len(c) < 20:
        print(f'  skip [short]: {c!r}', file=sys.stderr)
        continue
    key = c[:50]
    if key in seen: continue
    seen.add(key)
    s = m.get('relevance_score', '?')
    try: s = f'{float(s):.2f}'
    except: pass
    print(f'  [{s}] {c[:100]}', file=sys.stderr)
    print(f'  - {c}')
" "$TMPFILE" 2>"$TMPFILE.err")
    local rc=$?
    if [[ -f "$TMPFILE.err" ]]; then
        while IFS= read -r line; do
            _log "recall: $line"
        done < "$TMPFILE.err"
    fi
    rm -f "$TMPFILE.err"
    [[ $rc -ne 0 ]] && { _log "recall: no results"; return 1; }
    echo "$result"
}

# ============================================================
# Helper: Tool-specific query extraction
# ============================================================

extract_query() {
    local result
    result=$(python3 -c "
import json, sys
data = json.load(open(sys.argv[1]))
tool = data.get('tool_name', '')
ti = data.get('tool_input', {})
if tool in ('Read', 'Edit', 'Write'):
    fp = ti.get('file_path', '')
    parts = fp.rsplit('/', 2)
    print('/'.join(parts[-2:]) if len(parts) >= 2 else fp)
elif tool == 'Bash':
    print(ti.get('command', '')[:200])
elif tool == 'Task':
    print(ti.get('prompt', '')[:200])
elif tool == 'Grep':
    print(ti.get('pattern', ''))
else:
    print(ti.get('file_path', '') or ti.get('command', '') or ti.get('prompt', '') or ti.get('pattern', '') or '')
" "$INPUT_FILE" 2>/dev/null)
    _dbg "extract_query: '$result'"
    echo "$result"
}

# ============================================================
# Helper: JSON output formatters
# ============================================================

output_hook_context() {
    python3 -c "
import json, sys
print(json.dumps({
    'hookSpecificOutput': {
        'hookEventName': sys.argv[1],
        'additionalContext': sys.argv[2]
    }
}))" "$1" "$2" 2>/dev/null
}

output_top_context() {
    python3 -c "
import json, sys
print(json.dumps({'additionalContext': sys.argv[1]}))" "$1" 2>/dev/null
}

# ============================================================
# Event: SessionStart
# ============================================================

handle_session_start() {
    _metrics_timer_start
    local SOURCE
    SOURCE=$(python3 -c "import json; print(json.load(open('$INPUT_FILE')).get('source',''))" 2>/dev/null)
    _dbg "source=$SOURCE"

    # Clear dedup cache on new session or explicit clear
    if [[ "$SOURCE" == "startup" || "$SOURCE" == "clear" ]]; then
        dedup_clear
    fi

    # --- Cheat sheet recall ---
    local CHEAT_SHEET=""
    if wt-memory recall "cheat-sheet operational" --tags "cheat-sheet" --limit 5 2>/dev/null > "$TMPFILE"; then
        CHEAT_SHEET=$(python3 -c "
import sys, json
try:
    memories = json.load(open(sys.argv[1]))
except: sys.exit(0)
if not memories: sys.exit(0)
seen = set()
for m in memories:
    c = m.get('content','').replace('\n',' ').strip()
    if len(c) < 20: continue
    key = c[:50]
    if key in seen: continue
    seen.add(key)
    print(f'  - {c}')
" "$TMPFILE" 2>/dev/null)
    fi

    # --- Proactive project context (using git changed files, not commit messages) ---
    local PROJECT_CONTEXT=""
    local PROJECT_DIR="${CLAUDE_PROJECT_DIR:-$(pwd)}"
    local PROJECT_NAME
    PROJECT_NAME=$(basename "$PROJECT_DIR")

    local RECENT_FILES=""
    if git -C "$PROJECT_DIR" rev-parse --is-inside-work-tree &>/dev/null; then
        RECENT_FILES=$(git -C "$PROJECT_DIR" diff --name-only HEAD~5 HEAD 2>/dev/null | head -10 | tr '\n' ', ' | sed 's/,$//')
        [[ -z "$RECENT_FILES" ]] && RECENT_FILES=$(git -C "$PROJECT_DIR" diff --name-only 2>/dev/null | head -10 | tr '\n' ', ' | sed 's/,$//')
    fi

    local PROACTIVE_QUERY="Project: $PROJECT_NAME. Changed files: $RECENT_FILES"
    _dbg "proactive query: '${PROACTIVE_QUERY:0:120}'"
    PROJECT_CONTEXT=$(proactive_and_format "$PROACTIVE_QUERY" 5) || true

    # --- Build output ---
    local OUTPUT=""
    if [[ -n "$CHEAT_SHEET" ]]; then
        OUTPUT="=== OPERATIONAL CHEAT SHEET ===\n$CHEAT_SHEET"
        _dbg "cheat_sheet: $(echo "$CHEAT_SHEET" | wc -l) lines"
    else
        _dbg "cheat_sheet: empty"
    fi
    if [[ -n "$PROJECT_CONTEXT" ]]; then
        [[ -n "$OUTPUT" ]] && OUTPUT="$OUTPUT\n\n"
        OUTPUT="${OUTPUT}=== PROJECT CONTEXT ===\n$PROJECT_CONTEXT"
        _dbg "project_context: $(echo "$PROJECT_CONTEXT" | wc -l) lines"
    else
        _dbg "project_context: empty"
    fi

    if [[ -z "$OUTPUT" ]]; then
        _dbg "no output, exiting"
        local _dur; _dur=$(_metrics_timer_elapsed)
        _metrics_append "L1" "SessionStart" "$PROACTIVE_QUERY" 0 0 "[]" "$_dur" 0 0
        exit 0
    fi

    local _output_text
    _output_text=$(echo -e "$OUTPUT")
    local _tok_est=$(( ${#_output_text} / 4 ))
    local _dur; _dur=$(_metrics_timer_elapsed)
    local _scores; _scores=$(_extract_scores)
    local _res_count; _res_count=$(echo "$PROJECT_CONTEXT" | grep -c '^  - ' 2>/dev/null || echo 0)
    _metrics_append "L1" "SessionStart" "$PROACTIVE_QUERY" "$_res_count" "$_res_count" "$_scores" "$_dur" "$_tok_est" 0

    _dbg "=== OUTPUT ($(echo -e "$OUTPUT" | wc -c) bytes) ==="
    output_hook_context "SessionStart" "$_output_text"
}

# ============================================================
# Event: UserPromptSubmit
# ============================================================

handle_user_prompt() {
    _metrics_timer_start
    local PROMPT
    PROMPT=$(python3 -c "import json; print(json.load(open('$INPUT_FILE')).get('prompt',''))" 2>/dev/null)
    _dbg "prompt='${PROMPT:0:120}'"
    [[ -z "$PROMPT" ]] && { _dbg "empty prompt, exiting"; exit 0; }

    # --- Emotion detection ---
    local EMOTION_RESULT=""
    EMOTION_RESULT=$(python3 -c "
import json, sys
sys.path.insert(0, '$WT_TOOLS_ROOT')
from lib.frustration import detect

prompt = json.load(open(sys.argv[1])).get('prompt', '')

# Load session frustration history from dedup cache
cache_file = sys.argv[2]
history = {'count': 0, 'last_level': 'none'}
try:
    with open(cache_file) as f:
        cache = json.load(f)
        history = cache.get('frustration_history', history)
except: pass

result = detect(prompt, session_history=history)

# Save updated history back to cache
try:
    cache = {}
    try:
        with open(cache_file) as f:
            cache = json.load(f)
    except: pass
    cache['frustration_history'] = history
    with open(cache_file, 'w') as f:
        json.dump(cache, f)
except: pass

print(json.dumps(result))
" "$INPUT_FILE" "$CACHE_FILE" 2>/dev/null) || true

    local EMOTION_LEVEL="none"
    local EMOTION_INJECT=false
    local EMOTION_SAVE=false
    local EMOTION_TRIGGERS=""
    if [[ -n "$EMOTION_RESULT" ]]; then
        EMOTION_LEVEL=$(echo "$EMOTION_RESULT" | python3 -c "import json,sys; print(json.load(sys.stdin).get('level','none'))" 2>/dev/null || echo "none")
        EMOTION_INJECT=$(echo "$EMOTION_RESULT" | python3 -c "import json,sys; print(str(json.load(sys.stdin).get('inject',False)).lower())" 2>/dev/null || echo "false")
        EMOTION_SAVE=$(echo "$EMOTION_RESULT" | python3 -c "import json,sys; print(str(json.load(sys.stdin).get('save',False)).lower())" 2>/dev/null || echo "false")
        EMOTION_TRIGGERS=$(echo "$EMOTION_RESULT" | python3 -c "import json,sys; print(', '.join(json.load(sys.stdin).get('triggers',[])))" 2>/dev/null || echo "")
    fi
    _dbg "emotion: level=$EMOTION_LEVEL triggers=$EMOTION_TRIGGERS save=$EMOTION_SAVE inject=$EMOTION_INJECT"

    # Save memory on moderate/high
    if [[ "$EMOTION_SAVE" == "true" ]]; then
        local SAVE_TAGS="frustration,recurring,source:emotion-detect"
        local SAVE_PREFIX="âš ï¸ User frustrated (moderate)"
        if [[ "$EMOTION_LEVEL" == "high" ]]; then
            SAVE_TAGS="frustration,high-priority,source:emotion-detect"
            SAVE_PREFIX="ðŸ”´ User frustrated (high)"
        fi
        local SAVE_CONTENT="$SAVE_PREFIX: $(echo "$PROMPT" | head -c 500)"
        _log "remember: frustration type=Learning tags=$SAVE_TAGS"
        echo "$SAVE_CONTENT" | wt-memory remember --type Learning --tags "$SAVE_TAGS" 2>/dev/null || true
    fi

    # Extract change name from opsx/openspec skill invocation (not explore)
    local CHANGE_NAME=""
    CHANGE_NAME=$(echo "$PROMPT" | python3 -c "
import sys, re
prompt = sys.stdin.read()
CHANGE_SKILLS = r'(?:opsx:(?:apply|continue|verify|archive|sync|ff|new)|openspec-(?:apply|continue|verify|archive|sync|ff|new)[\w-]*)'
m = re.search(CHANGE_SKILLS + r'\s+(\S+)', prompt)
if m: print(m.group(1))
" 2>/dev/null)

    local QUERY=""
    if [[ -n "$CHANGE_NAME" ]]; then
        QUERY="$CHANGE_NAME $(echo "$PROMPT" | head -c 200)"
    else
        QUERY=$(echo "$PROMPT" | head -c 200)
    fi

    _dbg "change_name='$CHANGE_NAME' query='${QUERY:0:100}'"

    # Proactive recall (no MEMORY_COUNT==0 guard â€” fresh projects benefit from proactive)
    local FORMATTED=""
    FORMATTED=$(proactive_and_format "$QUERY" 5) || true

    # Load mandatory rules (deterministic, topic-matched, no shodh-memory dependency)
    local RULES_BLOCK=""
    RULES_BLOCK=$(load_matching_rules "$PROMPT") || true

    # Build output â€” mandatory rules + emotion warning + proactive recall
    local CONTEXT_TEXT=""

    # Inject mandatory rules first (highest priority)
    if [[ -n "$RULES_BLOCK" ]]; then
        CONTEXT_TEXT="$RULES_BLOCK"
    fi

    # Inject emotion warning if detected
    if [[ "$EMOTION_INJECT" == "true" ]]; then
        local EMOTION_WARNING=""
        if [[ "$EMOTION_LEVEL" == "high" ]]; then
            EMOTION_WARNING="âš  EMOTION DETECTED: The user appears strongly frustrated (triggers: $EMOTION_TRIGGERS). Acknowledge their concern directly. Be extra careful and avoid repeating previous mistakes."
        elif [[ "$EMOTION_LEVEL" == "moderate" ]]; then
            EMOTION_WARNING="âš  EMOTION DETECTED: The user appears frustrated (triggers: $EMOTION_TRIGGERS). Acknowledge their concern. Be extra careful with this task."
        else
            EMOTION_WARNING="Note: The user may be slightly frustrated (triggers: $EMOTION_TRIGGERS). Pay attention to their concern."
        fi
        if [[ -n "$CONTEXT_TEXT" ]]; then
            CONTEXT_TEXT="$CONTEXT_TEXT\n$EMOTION_WARNING"
        else
            CONTEXT_TEXT="$EMOTION_WARNING"
        fi
        _dbg "injected emotion warning"
    fi

    if [[ -n "$FORMATTED" ]]; then
        local MEMORY_SECTION="=== PROJECT MEMORY â€” If any memory below directly answers the user's question, cite it in your response ==="
        [[ -n "$CHANGE_NAME" ]] && MEMORY_SECTION="$MEMORY_SECTION\nChange: $CHANGE_NAME"
        MEMORY_SECTION="$MEMORY_SECTION\nRelevant past experience:\n$FORMATTED\n=== END ==="

        if [[ -n "$CONTEXT_TEXT" ]]; then
            CONTEXT_TEXT="$CONTEXT_TEXT\n$MEMORY_SECTION"
        else
            CONTEXT_TEXT="$MEMORY_SECTION"
        fi
    fi

    if [[ -z "$CONTEXT_TEXT" ]]; then
        _dbg "no output, exiting"
        local _dur; _dur=$(_metrics_timer_elapsed)
        _metrics_append "L2" "UserPromptSubmit" "${QUERY:0:200}" 0 0 "[]" "$_dur" 0 0
        exit 0
    fi

    local _tok_est=$(( ${#CONTEXT_TEXT} / 4 ))
    local _dur; _dur=$(_metrics_timer_elapsed)
    local _scores; _scores=$(_extract_scores)
    local _res_count; _res_count=$(echo "$FORMATTED" | grep -c '^  - ' 2>/dev/null || echo 0)
    _metrics_append "L2" "UserPromptSubmit" "${QUERY:0:200}" "$_res_count" "$_res_count" "$_scores" "$_dur" "$_tok_est" 0

    _dbg "=== OUTPUT ($(echo -e "$CONTEXT_TEXT" | wc -c) bytes) ==="
    output_hook_context "UserPromptSubmit" "$CONTEXT_TEXT"
}

# ============================================================
# Event: PreToolUse (disabled â€” memory recall removed)
# ============================================================

handle_pre_tool() {
    _dbg "PreToolUse disabled, exiting"
    exit 0
}

# ============================================================
# Event: PostToolUse (Read + Bash only, recall with dedup)
# ============================================================

handle_post_tool() {
    _metrics_timer_start
    local TOOL_NAME
    TOOL_NAME=$(python3 -c "import json; print(json.load(open('$INPUT_FILE')).get('tool_name',''))" 2>/dev/null)
    _dbg "tool=$TOOL_NAME"

    # Only process Read and Bash â€” exit immediately for all others
    if [[ "$TOOL_NAME" != "Read" && "$TOOL_NAME" != "Bash" ]]; then
        _dbg "tool=$TOOL_NAME not in scope, exiting"
        exit 0
    fi

    local QUERY
    QUERY=$(extract_query)
    if [[ -z "$QUERY" ]]; then
        _dbg "empty query, exiting"
        exit 0
    fi

    # Recall with dedup check
    local KEY
    KEY=$(make_dedup_key "PostToolUse" "$TOOL_NAME" "$QUERY")
    if dedup_check "$KEY"; then
        _dbg "dedup hit, exiting"
        local _dur; _dur=$(_metrics_timer_elapsed)
        _metrics_append "L3" "PostToolUse" "${QUERY:0:200}" 0 0 "[]" "$_dur" 0 1
        exit 0
    fi

    local FORMATTED=""
    FORMATTED=$(recall_and_format "$QUERY" 2 hybrid) || true
    if [[ -z "$FORMATTED" ]]; then
        _dbg "no recall results, exiting"
        local _dur; _dur=$(_metrics_timer_elapsed)
        local _scores; _scores=$(_extract_scores)
        _metrics_append "L3" "PostToolUse" "${QUERY:0:200}" 0 0 "$_scores" "$_dur" 0 0
        exit 0
    fi

    dedup_add "$KEY"
    local _output_text="=== MEMORY: Context for this file/command ===\n$FORMATTED"
    local _tok_est=$(( ${#_output_text} / 4 ))
    local _dur; _dur=$(_metrics_timer_elapsed)
    local _scores; _scores=$(_extract_scores)
    local _res_count; _res_count=$(echo "$FORMATTED" | grep -c '^  - ' 2>/dev/null || echo 0)
    _metrics_append "L3" "PostToolUse" "${QUERY:0:200}" "$_res_count" "$_res_count" "$_scores" "$_dur" "$_tok_est" 0

    _dbg "=== OUTPUT ==="
    output_hook_context "PostToolUse" "$_output_text"
}

# ============================================================
# Event: PostToolUseFailure (error recall)
# ============================================================

handle_post_tool_failure() {
    _metrics_timer_start
    local PARSED
    PARSED=$(python3 -c "
import json, sys
data = json.load(open(sys.argv[1]))
is_int = str(data.get('is_interrupt', False))
error = data.get('error', '')
print(f'{is_int}')
print(error[:300])
" "$INPUT_FILE" 2>/dev/null)

    local IS_INTERRUPT ERROR_TEXT
    IS_INTERRUPT=$(echo "$PARSED" | head -1)
    ERROR_TEXT=$(echo "$PARSED" | tail -n +2)

    _dbg "is_interrupt=$IS_INTERRUPT error='${ERROR_TEXT:0:80}'"

    if [[ "$IS_INTERRUPT" == "True" || "$IS_INTERRUPT" == "true" ]]; then
        _dbg "interrupt, exiting"
        exit 0
    fi
    if [[ ${#ERROR_TEXT} -lt 10 ]]; then
        _dbg "error too short (${#ERROR_TEXT} chars), exiting"
        exit 0
    fi

    local FORMATTED=""
    FORMATTED=$(recall_and_format "$ERROR_TEXT" 3 hybrid) || true
    if [[ -z "$FORMATTED" ]]; then
        _dbg "no recall results for error, exiting"
        local _dur; _dur=$(_metrics_timer_elapsed)
        local _scores; _scores=$(_extract_scores)
        _metrics_append "L4" "PostToolUseFailure" "${ERROR_TEXT:0:200}" 0 0 "$_scores" "$_dur" 0 0
        exit 0
    fi

    local _output_text="=== MEMORY: Past fix for this error ===\n$FORMATTED"
    local _tok_est=$(( ${#_output_text} / 4 ))
    local _dur; _dur=$(_metrics_timer_elapsed)
    local _scores; _scores=$(_extract_scores)
    local _res_count; _res_count=$(echo "$FORMATTED" | grep -c '^  - ' 2>/dev/null || echo 0)
    _metrics_append "L4" "PostToolUseFailure" "${ERROR_TEXT:0:200}" "$_res_count" "$_res_count" "$_scores" "$_dur" "$_tok_est" 0

    _dbg "=== OUTPUT ==="
    output_hook_context "PostToolUseFailure" "$_output_text"
}

# ============================================================
# Event: SubagentStop
# ============================================================

handle_subagent_stop() {
    local AGENT_PATH
    AGENT_PATH=$(python3 -c "import json; print(json.load(open('$INPUT_FILE')).get('agent_transcript_path',''))" 2>/dev/null)
    _dbg "agent_path='$AGENT_PATH'"
    if [[ -z "$AGENT_PATH" ]]; then
        _dbg "no agent path, exiting"
        exit 0
    fi

    # Expand ~ in path
    AGENT_PATH="${AGENT_PATH/#\~/$HOME}"
    if [[ ! -f "$AGENT_PATH" ]]; then
        _dbg "agent transcript not found: $AGENT_PATH"
        exit 0
    fi

    # Extract last few assistant text entries as query
    local SUMMARY
    SUMMARY=$(python3 -c "
import json, sys
entries = []
try:
    with open(sys.argv[1]) as f:
        for line in f:
            try:
                obj = json.loads(line)
                if obj.get('type') == 'assistant':
                    for block in (obj.get('message',{}).get('content',[]) or []):
                        if isinstance(block, dict) and block.get('type') == 'text':
                            entries.append(block.get('text','')[:200])
            except: pass
except: pass
print(' '.join(entries[-3:])[:500])
" "$AGENT_PATH" 2>/dev/null)
    _dbg "summary='${SUMMARY:0:100}'"
    if [[ -z "$SUMMARY" ]]; then
        _dbg "empty summary, exiting"
        exit 0
    fi

    local FORMATTED=""
    FORMATTED=$(proactive_and_format "$SUMMARY" 2) || true
    if [[ -z "$FORMATTED" ]]; then
        _dbg "no proactive results, exiting"
        exit 0
    fi

    _dbg "=== OUTPUT ==="
    output_hook_context "SubagentStop" "=== MEMORY: Context from subagent ===\n$FORMATTED"
}

# ============================================================
# Event: Stop (transcript extraction + commit-based extraction)
# ============================================================

handle_stop() {
    local STOP_ACTIVE
    STOP_ACTIVE=$(python3 -c "import json; print(json.load(open('$INPUT_FILE')).get('stop_hook_active',False))" 2>/dev/null)
    _dbg "stop_active=$STOP_ACTIVE"
    if [[ "$STOP_ACTIVE" == "True" || "$STOP_ACTIVE" == "true" ]]; then
        _dbg "stop hook already active, exiting"
        exit 0
    fi

    # Flush metrics to SQLite before clearing dedup cache
    _stop_flush_metrics

    # Clean up dedup cache
    dedup_clear

    local TRANSCRIPT_PATH
    TRANSCRIPT_PATH=$(python3 -c "import json; print(json.load(open('$INPUT_FILE')).get('transcript_path',''))" 2>/dev/null)
    TRANSCRIPT_PATH="${TRANSCRIPT_PATH/#\~/$HOME}"
    _dbg "transcript='$TRANSCRIPT_PATH'"

    mkdir -p .wt-tools

    # Background transcript extraction
    if [[ -n "$TRANSCRIPT_PATH" && -f "$TRANSCRIPT_PATH" ]]; then
        _dbg "launching background extraction"
        _stop_run_extraction_bg "$TRANSCRIPT_PATH" &
        disown
    else
        _dbg "no transcript file, skipping extraction"
    fi

    # Synchronous: commit-based extraction
    _dbg "running commit extraction"
    _stop_commit_extraction
    _dbg "=== DONE ==="
}

# --- Stop: Flush metrics to SQLite ---

_stop_flush_metrics() {
    [[ "$METRICS_ENABLED" -eq 0 ]] && return 0
    [[ ! -f "$CACHE_FILE" ]] && { _dbg "metrics: no cache file"; return 0; }

    local TRANSCRIPT_PATH
    TRANSCRIPT_PATH=$(python3 -c "import json; print(json.load(open('$INPUT_FILE')).get('transcript_path',''))" 2>/dev/null)
    TRANSCRIPT_PATH="${TRANSCRIPT_PATH/#\~/$HOME}"

    python3 -c "
import sys, json, os
sys.path.insert(0, '$WT_TOOLS_ROOT')
from lib.metrics import flush_session, scan_transcript_citations

cache_file = sys.argv[1]
session_id = sys.argv[2]
transcript_path = sys.argv[3]

# Read metrics from session cache
try:
    with open(cache_file) as f:
        cache = json.load(f)
except Exception:
    sys.exit(0)

metrics = cache.get('_metrics', [])
if not metrics:
    sys.exit(0)

# Resolve project name
project = 'unknown'
try:
    import subprocess
    result = subprocess.run(['git', 'rev-parse', '--show-toplevel'],
                          capture_output=True, text=True, timeout=5)
    if result.returncode == 0:
        project = os.path.basename(result.stdout.strip())
except Exception:
    pass

# Scan transcript for citations
citations = []
if transcript_path and os.path.exists(transcript_path):
    citations = scan_transcript_citations(transcript_path, session_id)

# Flush to SQLite
flush_session(session_id, project, metrics, citations)
print(f'Flushed {len(metrics)} metrics, {len(citations)} citations', file=sys.stderr)
" "$CACHE_FILE" "$SESSION_ID" "$TRANSCRIPT_PATH" 2>/dev/null || true
    _log "metrics: flushed to SQLite"
}

# --- Stop: Background raw transcript filter ---

STOP_LOCK_FILE=".wt-tools/.transcript-extraction.lock"
STOP_LOG_FILE=".wt-tools/transcript-extraction.log"

_stop_extract_change_names() {
    local transcript="$1"
    python3 -c "
import json, sys
names = set()
with open(sys.argv[1]) as f:
    for line in f:
        try:
            obj = json.loads(line)
            if obj.get('type') != 'assistant': continue
            for block in (obj.get('message',{}).get('content',[]) or []):
                if not isinstance(block, dict): continue
                if block.get('type') == 'tool_use' and block.get('name') == 'Skill':
                    inp = block.get('input',{})
                    skill = inp.get('skill','')
                    if 'opsx:' in skill or 'openspec-' in skill:
                        args = inp.get('args','').strip()
                        if args:
                            names.add(args.split()[0])
        except: pass
print(','.join(sorted(names)[:5]) if names else '')
" "$transcript" 2>/dev/null
}

_stop_raw_filter() {
    local transcript="$1"
    [[ ! -f "$transcript" ]] && return 1

    # Extract change names (reuse existing logic)
    local change_names
    change_names=$(_stop_extract_change_names "$transcript")
    local first_change="unknown"
    [[ -n "$change_names" ]] && first_change="${change_names%%,*}"

    # Parse and filter transcript â€” returns JSON array of {type, content} objects
    local filtered_json
    filtered_json=$(python3 << 'PYEOF'
import json, sys, os

transcript = sys.argv[1]
entries = []
file_read_counts = {}

with open(transcript) as f:
    for line in f:
        try:
            obj = json.loads(line)
            t = obj.get('type', '')

            if t == 'user':
                msg = obj.get('message', {})
                content = msg.get('content', '')
                if isinstance(content, str):
                    # Filter system/command noise
                    import re
                    if '<system-reminder>' in content:
                        content = re.sub(r'<system-reminder>.*?</system-reminder>', '', content, flags=re.DOTALL).strip()
                    if '<local-command' in content or '<command-name>' in content or '<command-message>' in content:
                        content = re.sub(r'<(?:local-command[\w-]*|command-name|command-message|command-args)>.*?</(?:local-command[\w-]*|command-name|command-message|command-args)>', '', content, flags=re.DOTALL).strip()
                    if len(content) >= 15:
                        entries.append({'role': 'user', 'content': content[:2000]})

            elif t == 'assistant':
                msg = obj.get('message', {})
                content_blocks = msg.get('content', [])
                if isinstance(content_blocks, list):
                    for block in content_blocks:
                        if not isinstance(block, dict):
                            continue
                        if block.get('type') == 'text':
                            text = block.get('text', '').strip()
                            if len(text) >= 50:
                                entries.append({'role': 'assistant', 'content': text[:2000]})
                        elif block.get('type') == 'tool_use':
                            name = block.get('name', '')
                            inp = block.get('input', {})
                            # Track file reads for dedup
                            if name == 'Read':
                                fp = inp.get('file_path', '')
                                file_read_counts[fp] = file_read_counts.get(fp, 0) + 1
                                if file_read_counts[fp] > 2:
                                    continue
                            # Keep Bash commands with output context
                            if name == 'Bash':
                                cmd = inp.get('command', '')[:200]
                                if cmd:
                                    entries.append({'role': 'assistant', 'content': f'[Bash] {cmd}'})

            elif t == 'tool_result':
                content = obj.get('content', '')
                if isinstance(content, str):
                    cl = content.lower()
                    if ('error' in cl or 'failed' in cl or 'traceback' in cl) and len(content) >= 15:
                        entries.append({'role': 'assistant', 'content': f'[Error] {content[:500]}'})
        except:
            pass

print(json.dumps(entries))
PYEOF
    "$transcript")

    [[ -z "$filtered_json" || "$filtered_json" == "[]" ]] && return 0

    # Write filtered entries to temp file to avoid shell quoting issues
    local entries_file
    entries_file=$(mktemp)
    echo "$filtered_json" > "$entries_file"

    # Save filtered turns with context prefix and tags
    python3 -c "
import json, sys, subprocess

entries = json.load(open(sys.argv[1]))
if not entries:
    sys.exit(0)

change_name = sys.argv[2]
total = len(entries)
change_tags = f'change:{change_name}' if change_name != 'unknown' else ''
base_tags = 'raw,phase:auto-extract,source:hook'
if change_tags:
    base_tags = f'{base_tags},{change_tags}'

saved = 0
for i, entry in enumerate(entries, 1):
    role = entry['role']
    content = entry['content']
    prefix = f'[session:{change_name}, turn {i}/{total}] '
    full_content = prefix + content

    mem_type = 'Context' if role == 'user' else 'Learning'

    try:
        subprocess.run(
            ['wt-memory', 'remember', '--type', mem_type, '--tags', base_tags],
            input=full_content, text=True, capture_output=True, timeout=5
        )
        saved += 1
    except Exception:
        pass

print(f'saved={saved}/{total}', file=sys.stderr)
" "$entries_file" "$first_change" 2>> "$STOP_LOG_FILE" || true
    rm -f "$entries_file"

    _log "raw-filter: saved entries for change=$first_change from transcript"
    return 0
}

# One-time migration: commit any existing staged files from Haiku era
_stop_migrate_staged() {
    local found=0
    for staged in .wt-tools/.staged-extract-*; do
        [[ -f "$staged" ]] || continue
        [[ "$staged" == *.ts ]] && continue
        found=1

        local first_change=""
        local first_line
        first_line=$(head -1 "$staged")
        [[ "$first_line" == "#CHANGE:"* ]] && first_change="${first_line#\#CHANGE:}"

        local count=0 conv_count=0 cheat_count=0
        while IFS= read -r line; do
            [[ -z "$line" ]] && continue
            [[ "$line" == "NONE" ]] && continue
            [[ "$line" == "#CHANGE:"* ]] && continue
            [[ "$line" != *"|"*"|"* ]] && continue

            local mem_type="${line%%|*}"
            local rest="${line#*|}"
            local tags="${rest%%|*}"
            local content="${rest#*|}"

            if [[ "$mem_type" == "Convention" ]]; then
                (( conv_count >= 2 )) && continue
                mem_type="Learning"
                tags="convention,cheat-sheet,$tags"
                (( conv_count++ ))
            elif [[ "$mem_type" == "CheatSheet" ]]; then
                (( cheat_count >= 2 )) && continue
                mem_type="Learning"
                tags="cheat-sheet,$tags"
                (( cheat_count++ ))
            else
                (( count >= 5 )) && continue
                case "$mem_type" in
                    Learning|Decision|Context) ;;
                    *) continue ;;
                esac
                (( count++ ))
            fi

            [[ -z "$content" ]] && continue

            local full_tags="phase:auto-extract,source:hook,$tags"
            [[ -n "$first_change" ]] && full_tags="change:$first_change,$full_tags"

            _log "migrate: committing staged type=$mem_type content='${content:0:80}'"
            echo "$content" | wt-memory remember --type "$mem_type" --tags "$full_tags" 2>/dev/null || true
        done < "$staged"

        local ts_file="${staged}.ts"
        rm -f "$staged" "$ts_file"
    done
    [[ "$found" -eq 1 ]] && _log "migrate: staged files committed"
}

_stop_run_extraction_bg() {
    local transcript="$1"

    # Lockfile check
    if [[ -f "$STOP_LOCK_FILE" ]]; then
        local existing_pid
        existing_pid=$(cat "$STOP_LOCK_FILE" 2>/dev/null)
        if [[ -n "$existing_pid" ]] && kill -0 "$existing_pid" 2>/dev/null; then
            return 0
        fi
        rm -f "$STOP_LOCK_FILE"
    fi

    echo ${BASHPID:-$$} > "$STOP_LOCK_FILE"
    trap 'rm -f "$STOP_LOCK_FILE"' EXIT

    # One-time migration of old Haiku staged files
    _stop_migrate_staged

    echo "[$(date '+%Y-%m-%d %H:%M:%S')] Starting raw filter for: $transcript" >> "$STOP_LOG_FILE"
    _stop_raw_filter "$transcript" 2>> "$STOP_LOG_FILE" || true
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] Raw filter complete" >> "$STOP_LOG_FILE"

    rm -f "$STOP_LOCK_FILE"
}

# --- Stop: Synchronous commit-based extraction ---

_stop_commit_extraction() {
    local MARKER_FILE=".wt-tools/.last-memory-commit"
    local DESIGN_MARKER=".wt-tools/.saved-designs"
    local CODEMAP_MARKER=".wt-tools/.saved-codemaps"

    local LAST_HASH=""
    [[ -f "$MARKER_FILE" ]] && LAST_HASH=$(cat "$MARKER_FILE" 2>/dev/null)

    local CURRENT_HASH
    CURRENT_HASH=$(git rev-parse HEAD 2>/dev/null) || return 0
    [[ "$CURRENT_HASH" == "$LAST_HASH" ]] && return 0

    local COMMITS
    if [[ -n "$LAST_HASH" ]] && git cat-file -t "$LAST_HASH" &>/dev/null; then
        COMMITS=$(git log --oneline "$LAST_HASH..HEAD" 2>/dev/null)
    else
        COMMITS=$(git log --oneline -1 2>/dev/null)
    fi

    while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        local hash="${line%% *}"
        local msg="${line#* }"

        local change_name="general"
        [[ "$msg" == *:* ]] && change_name="${msg%%:*}"

        if [[ "$change_name" == "general" ]] && [[ -d "openspec/changes" ]]; then
            for d in openspec/changes/*/; do
                local dname
                dname=$(basename "$d")
                [[ "$dname" == "archive" ]] && continue
                change_name="$dname"
                break
            done
        fi

        # Code map safety net
        touch "$CODEMAP_MARKER" 2>/dev/null
        if ! grep -qx "$change_name" "$CODEMAP_MARKER" 2>/dev/null; then
            local has_codemap
            has_codemap=$(wt-memory recall "$change_name code map" --limit 1 --mode semantic 2>/dev/null \
                | python3 -c "import sys,json; r=json.load(sys.stdin); print('yes' if any('code-map' in ','.join(m.get('tags',[])) for m in r) else 'no')" 2>/dev/null)

            if [[ "$has_codemap" != "yes" ]]; then
                local all_hashes
                all_hashes=$(echo "$COMMITS" | awk '{print $1}')
                local changed_files
                changed_files=$(echo "$all_hashes" | while read -r h; do
                    [[ -z "$h" ]] && continue
                    git diff-tree --no-commit-id --name-only -r "$h" 2>/dev/null
                done \
                    | sort -u \
                    | grep -vE '(package\.json|package-lock|\.config\.|tsconfig|\.test\.|\.spec\.|__test__)' \
                    | head -8 \
                    | tr '\n' ', ' \
                    | sed 's/,$//')

                if [[ -n "$changed_files" ]]; then
                    local codemap_content="$change_name code map (auto): $changed_files"
                    [[ ${#codemap_content} -gt 400 ]] && codemap_content="${codemap_content:0:397}..."
                    _log "remember: codemap change=$change_name files=${changed_files:0:80}"
                    echo "$codemap_content" | wt-memory remember --type Context --tags "change:$change_name,phase:apply,source:hook,code-map" 2>/dev/null || true
                fi
            fi
            echo "$change_name" >> "$CODEMAP_MARKER"
        fi

        # Design choice extraction
        touch "$DESIGN_MARKER" 2>/dev/null
        grep -qx "$change_name" "$DESIGN_MARKER" 2>/dev/null && continue

        local design_file="openspec/changes/$change_name/design.md"
        if [[ -f "$design_file" ]]; then
            local choices
            choices=$(grep '^\*\*Choice\*\*' "$design_file" 2>/dev/null \
                | sed 's/^\*\*Choice\*\*: //' \
                | sed 's/^\*\*Choice\*\*://' \
                | tr '\n' '.' \
                | sed 's/\.\./\. /g; s/\.$//')

            if [[ -n "$choices" ]]; then
                local content="$change_name: $choices"
                [[ ${#content} -gt 300 ]] && content="${content:0:297}..."
                _log "remember: design-choices change=$change_name content='${content:0:80}'"
                echo "$content" | wt-memory remember --type Decision --tags "change:$change_name,phase:apply,source:hook,decisions" 2>/dev/null || true
            fi
        fi
        echo "$change_name" >> "$DESIGN_MARKER"
    done <<< "$COMMITS"

    echo "$CURRENT_HASH" > "$MARKER_FILE"
}

# ============================================================
# Dispatch
# ============================================================

_log "dispatch"
_dbg "dispatching event=$EVENT"
case "$EVENT" in
    SessionStart)       handle_session_start ;;
    UserPromptSubmit)   handle_user_prompt ;;
    PreToolUse)         handle_pre_tool ;;
    PostToolUse)        handle_post_tool ;;
    PostToolUseFailure) handle_post_tool_failure ;;
    SubagentStop)       handle_subagent_stop ;;
    Stop)               handle_stop ;;
    *)                  _log "unknown event, skipping"; exit 0 ;;
esac
_log "done"
_dbg "=== END ==="
